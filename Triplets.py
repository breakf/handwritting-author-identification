{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def valid_triplets_mask(labels):\n",
    "    \"\"\"Compute the 3D boolean mask where mask[a, p, n] is True if (a, p, n) is a valid triplet,\n",
    "    as in a, p, n are distinct and labels[a] == labels[p], labels[a] != labels[n].\n",
    "\n",
    "    :param labels: tensor of shape (batch_size,)\n",
    "    :return mask: tf.bool tensor of shape (batch_size, batch_size, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def euclidean_distance(embeddings, squared=False):\n",
    "    \"\"\"Computes pairwise euclidean distance matrix with numerical stability.\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    :param embeddings: 2-D Tensor of size [number of data, feature dimension].\n",
    "    :param squared: Boolean, whether or not to square the pairwise distances.\n",
    "    :return dist: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    dist_squared = tf.add(tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True),\n",
    "                          tf.reduce_sum(tf.square(tf.transpose(embeddings)), axis=0, keepdims=True)\n",
    "                          ) - 2.0 * tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    dist_squared = tf.maximum(dist_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = tf.less_equal(dist_squared, 0.0)\n",
    "    # Optionally take the sqrt.\n",
    "    dist = dist_squared if squared else tf.sqrt(dist_squared + tf.cast(error_mask, dtype=tf.float32) * 1e-16)\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    dist = tf.multiply(dist, tf.cast(tf.logical_not(error_mask), dtype=tf.float32))\n",
    "\n",
    "    n_data = tf.shape(embeddings)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = tf.ones_like(dist) - tf.linalg.diag(tf.ones([n_data]))\n",
    "    dist = tf.multiply(dist, mask_offdiagonals)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the maximum.\n",
    "    :return masked_maximums: N-D `Tensor`. The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = tf.reduce_min(data, axis=dim, keepdims=True)\n",
    "    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), axis=dim, keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the minimum.\n",
    "    :return masked_minimums: N-D `Tensor`. The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = tf.reduce_max(data, axis=dim, keepdims=True)\n",
    "    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), axis=dim, keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "\n",
    "\n",
    "def triplet_loss(margin=1.0, strategy='batch_semi_hard'):\n",
    "    \"\"\"Compute the triplet loss over the batch of embeddings. tf contrib inspired:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/losses/python/metric_learning/metric_loss_ops.py\n",
    "\n",
    "    :param margin: margin that is going to be enforced by the triplet loss\n",
    "    :param strategy: string, that indicated whether we're using the 'batch hard', 'batch all' or 'batch_semi_hard' mining strategy\n",
    "    :return: a callback function that calculates the loss according to the specified strategy\n",
    "    \"\"\"\n",
    "    def get_loss_tensor(positive_dists, negative_dists):\n",
    "        \"\"\"Compute the triplet loss function tensor using specified margin:\n",
    "\n",
    "        :param positive_dists: positive distances tensor\n",
    "        :param negative_dists:  negative distances tensor\n",
    "        :return: resulting triplet loss tensor\n",
    "        \"\"\"\n",
    "        if margin == 'soft':\n",
    "            return tf.nn.softplus(positive_dists - negative_dists)\n",
    "\n",
    "        return tf.maximum(positive_dists - negative_dists + margin, 0.0)\n",
    "\n",
    "    def batch_semi_hard(labels, embeddings):\n",
    "        \"\"\"Computes the triplet loss with semi-hard negative mining.\n",
    "        The loss encourages the positive distances (between a pair of embeddings with\n",
    "        the same labels) to be smaller than the minimum negative distance among\n",
    "        which are at least greater than the positive distance plus the margin constant\n",
    "        (called semi-hard negative) in the mini-batch. If no such negative exists,\n",
    "        uses the largest negative distance instead.\n",
    "        See: https://arxiv.org/abs/1503.03832.\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        batch_size = tf.size(labels)\n",
    "        # Build pairwise squared distance matrix.\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        # Build pairwise binary adjacency matrix (equal label mask).\n",
    "        adjacency = tf.equal(labels, tf.transpose(labels))\n",
    "        # Invert so we can select negatives only.\n",
    "        adjacency_not = tf.logical_not(adjacency)\n",
    "\n",
    "        # Compute the mask.\n",
    "        dist_tile = tf.tile(dist, [batch_size, 1])  # stack dist matrix batch_size times, axis=0\n",
    "        mask = tf.logical_and(tf.tile(adjacency_not, [batch_size, 1]), tf.greater(dist_tile, tf.reshape(dist, [-1, 1])))\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        is_negatives_outside = tf.reshape(tf.greater(tf.reduce_sum(mask, axis=1, keepdims=True), 0.0), [batch_size, batch_size])\n",
    "        is_negatives_outside = tf.transpose(is_negatives_outside)\n",
    "\n",
    "        # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "        negatives_outside = tf.reshape(masked_minimum(dist_tile, mask), [batch_size, batch_size])\n",
    "        negatives_outside = tf.transpose(negatives_outside)\n",
    "\n",
    "        # negatives_inside: largest D_an.\n",
    "        adjacency_not = tf.cast(adjacency_not, dtype=tf.float32)\n",
    "        negatives_inside = tf.tile(masked_maximum(dist, adjacency_not), [1, batch_size])\n",
    "\n",
    "        semi_hard_negatives = tf.where(is_negatives_outside, negatives_outside, negatives_inside)\n",
    "\n",
    "        # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "        #   in semihard, they take all positive pairs except the diagonal.\n",
    "        mask_positives = tf.cast(adjacency, dtype=tf.float32) - tf.linalg.diag(tf.ones([batch_size]))\n",
    "        n_positives = tf.reduce_sum(mask_positives)\n",
    "\n",
    "        loss_mat = get_loss_tensor(dist, semi_hard_negatives)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(tf.multiply(loss_mat, mask_positives)), n_positives)\n",
    "        return loss\n",
    "\n",
    "    def batch_all(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating all the valid triplets and averaging over the positive ones\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        #mask = tf.to_float(valid_triplets_mask(labels))\n",
    "        mask = tf.cast(valid_triplets_mask(labels), dtype=tf.float32)\n",
    "\n",
    "        anchor_positive_dist = tf.expand_dims(dist, 2)\n",
    "        anchor_negative_dist = tf.expand_dims(dist, 1)\n",
    "\n",
    "        loss_tensor = get_loss_tensor(anchor_positive_dist, anchor_negative_dist)\n",
    "        loss_tensor = tf.multiply(loss_tensor, mask)\n",
    "\n",
    "        #num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_tensor, 1e-16)))\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.cast(tf.greater(loss_tensor, 1e-16), dtype=tf.float32))\n",
    "        #loss = tf.div_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    def batch_hard(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating only hardest valid triplets and averaging over the positive ones.\n",
    "        One triplet per embedding, i.e. per anchor\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        adjacency = tf.cast(tf.equal(tf.reshape(labels, (-1, 1)), tf.reshape(labels, (1, -1))), tf.float32)\n",
    "\n",
    "        pos_dist = tf.reduce_max(adjacency * dist, axis=1)\n",
    "        inf = tf.constant(1e+9, tf.float32)\n",
    "        neg_dist = tf.reduce_min((adjacency * inf) + dist, axis=1)\n",
    "\n",
    "        loss_mat = get_loss_tensor(pos_dist, neg_dist)\n",
    "\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_mat, 1e-16)))\n",
    "        loss = tf.div_no_nan(tf.reduce_sum(loss_mat), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    if strategy == 'batch_semi_hard':\n",
    "        return batch_semi_hard\n",
    "    elif strategy == 'batch hard':\n",
    "        return batch_hard\n",
    "    else:\n",
    "        return batch_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def fetch(img_dir, name):\n",
    "    #print('image ' + str(name))\n",
    "    img = cv2.imread(join(img_dir, name))\n",
    "    if img.shape == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    elif img.shape == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize(img, size=(1024, 768)):\n",
    "    assert len(size) == 2\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def pad(img, size=(1024, 768)):\n",
    "    assert len(img.shape) == 3\n",
    "    assert len(size) == 2\n",
    "    h, w, _ = img.shape\n",
    "    #assert w <= size[0] and h <= size[1]\n",
    "    pad_vert = np.ceil((size[1]-h) / 2).astype(np.uint32)\n",
    "    pad_hor = np.ceil((size[0]-w) / 2).astype(np.uint32)\n",
    "\n",
    "    padded = np.full((size[1], size[0], 3), 255).astype(np.uint8)\n",
    "    padded[pad_vert:pad_vert+h, pad_hor:pad_hor+w, :] = img.copy()\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "class WordsSequence(Sequence):\n",
    "    def __init__(self, img_dir, input_shape, x_set, y_set=None, batch_size=16):\n",
    "        if y_set is not None:\n",
    "            self.x, self.y = x_set, y_set\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "        else:\n",
    "            self.x, self.y = x_set, None\n",
    "            \n",
    "        self.img_dir = img_dir\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x])\n",
    "\n",
    "        unused = self.dataset.loc[self.dataset['used'] == 0]\n",
    "            \n",
    "        if len(unused) >= self.batch_size:\n",
    "            batch_indices = unused.sample(n=self.batch_size).index\n",
    "        else:\n",
    "            batch_indices = unused.sample(n=self.batch_size, replace=True).index\n",
    "\n",
    "        self.dataset.loc[batch_indices, 'used'] = 1\n",
    "        batch_x = self.dataset.iloc[batch_indices]['x'].values\n",
    "        batch_y = self.dataset.iloc[batch_indices]['y'].values\n",
    "        return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        assert len(img.shape) == 3\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        if h / w <= self.input_shape[0] / self.input_shape[1]:\n",
    "            img = resize(img, (self.input_shape[1], int(self.input_shape[1] * h / w)))\n",
    "        else:\n",
    "            img = resize(img, (int(self.input_shape[0] * w / h), self.input_shape[0]))\n",
    "\n",
    "        img = pad(img, (self.input_shape[1], self.input_shape[0]))\n",
    "        return img / 255.  \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.y is not None:\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import  sample\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "\n",
    "\n",
    "def get_str2numb_numb2dict(vect):\n",
    "    str_to_ind_dict = {}\n",
    "    count = 0\n",
    "    for v in vect:\n",
    "        if v not in str_to_ind_dict.keys():\n",
    "            str_to_ind_dict[v] = count\n",
    "            count += 1\n",
    "    reverse_dict = {v:k for k, v in str_to_ind_dict.items()}\n",
    "    return str_to_ind_dict, reverse_dict\n",
    "\n",
    "def apply_dict(dict_keys, X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        res.append(dict_keys[x])\n",
    "    return res\n",
    "\n",
    "class ProgbarLossLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super(ProgbarLossLogger, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.seen = 0\n",
    "        self.target = self.params['steps']\n",
    "\n",
    "        if self.epochs > 1:\n",
    "            print('Epoch %d/%d' % (epoch + 1, self.epochs))\n",
    "        self.progbar = Progbar(target=self.target, verbose=True, stateful_metrics=['loss'])\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.seen < self.target:\n",
    "            self.log_values = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        num_steps = logs.get('num_steps', 1)\n",
    "        self.seen += num_steps\n",
    "\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.log_values.append((k, logs[k]))\n",
    "        self.progbar.update(self.seen, self.log_values)\n",
    "        \n",
    "class TripletModel:\n",
    "    def __init__(self, alpha, input_shape, cache_dir):\n",
    "        self.alpha = alpha\n",
    "        self.input_shape = input_shape\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self.model = self.build_model()\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        base_network = MobileNet(input_shape=self.input_shape, alpha=self.alpha, weights='imagenet', include_top=False, \n",
    "                                 pooling='avg')\n",
    "        x = Dense(128)(base_network.output)\n",
    "        x = Lambda(lambda x: K.l2_normalize(x, axis=1))(x)\n",
    "        model = Model(inputs=base_network.input, outputs=x)\n",
    "        model.summary()\n",
    "        return model\n",
    "           \n",
    "    def train(self, train_dir, train_csv, validation_dir, validation_csv, epochs, batch_size=32, learning_rate=0.001, margin=0.5):\n",
    "        train = pd.read_csv(train_csv)\n",
    "        # validation = pd.read_csv(validation_csv)\n",
    "        x_train, y_train = train['file_name'].as_matrix(), train['label'].as_matrix()\n",
    "        # x_validation, y_validation = validation['file_name'].as_matrix(), validation['label'].as_matrix()\n",
    "        \n",
    "        str2ind_train_dict, ind2str_train_dict = get_str2numb_numb2dict(y_train)\n",
    "        y_train = np.array(apply_dict(str2ind_train_dict, y_train))\n",
    "\n",
    "        # str2ind_val_dict, ind2str_val_dict = get_str2numb_numb2dict(y_validation)\n",
    "        # y_validation = np.array(apply_dict(str2ind_val_dict, y_validation))\n",
    "        \n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        train_generator = WordsSequence(train_dir, input_shape=self.input_shape, x_set=x_train, y_set=y_train, batch_size=batch_size)\n",
    "        # validation_generator = WordsSequence(validation_dir, input_shape=self.input_shape, x_set=validation_pairs, y_set=validation_y, batch_size=batch_size)\n",
    "\n",
    "        # optimize = RMSprop(lr=learning_rate)\n",
    "        optimize = Adam(lr=0.00001)\n",
    "        self.model.summary()\n",
    "        self.model.compile(loss=triplet_loss.triplet_loss(margin=1.0, strategy=\"batch_all\"), optimizer=optimize)\n",
    "        \n",
    "        # validation_data=validation_generator, \n",
    "        self.model.fit_generator(train_generator, shuffle=True, epochs=epochs, verbose=1, \n",
    "        callbacks=[ModelCheckpoint(filepath=os.path.join(self.cache_dir, 'checkpoint-{epoch:02d}.h5'), save_weights_only=True)])\n",
    "        \n",
    "        self.model.save('final_model.h5')\n",
    "        self.save_weights('final_weights.h5')\n",
    "\n",
    "\n",
    "    def save_embeddings(self, filename):\n",
    "        self.embeddings.to_pickle(filename)\n",
    "    \n",
    "    def load_embeddings(self, filename):\n",
    "        self.embeddings = pd.read_pickle(filename)    \n",
    "        \n",
    "    def save_weights(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        self.model.load_weights(filename, by_name=True, skip_mismatch=True)\n",
    "        \n",
    "    \n",
    "    def make_embeddings(self, img_dir, csv, batch_size=32):\n",
    "        if self.embeddings is not None:\n",
    "            print(self.embeddings[0][0])\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "        else:\n",
    "            data = pd.read_csv(csv)\n",
    "            x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "            \n",
    "            self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "            y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "            words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "            pred = self.model.predict_generator(words, verbose=1)\n",
    "\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(pred, y) \n",
    "     \n",
    "            self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "            self.save_embeddings('embeddings.pkl')\n",
    "    \n",
    "    def predict(self, img_dir, test_csv, batch_size=32):\n",
    "        self.model.summary()\n",
    "        test = pd.read_csv(test_csv)\n",
    "        x_test, y_test = test['file_name'].as_matrix(), test['label'].as_matrix()\n",
    "        \n",
    "        str2ind_test_dict, ind2str_test_dict = get_str2numb_numb2dict(y_test)\n",
    "        test_y = np.array(apply_dict(str2ind_test_dict, y_test))\n",
    "\n",
    "        words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x_test, batch_size=batch_size)\n",
    "        test_embeddings = self.model.predict_generator(words, verbose=1)\n",
    " \n",
    "        res = self.clf.predict(test_embeddings) \n",
    "        predict = np.array(apply_dict(ind2str_test_dict , res))\n",
    "        count = 0\n",
    "        for i,j in zip(predict, y_test):\n",
    "            if i == j:\n",
    "                count += 1\n",
    "\n",
    "        print('word accuracy: ', count / len(y_test))\n",
    "        \n",
    "        count = 0\n",
    "        autors = np.unique(y_test)\n",
    "        autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "            if p == autors[i]:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    "        \n",
    "        count = 0\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = [pair[0] for pair in Counter(np.ravel(predict[inds])).most_common(5)]\n",
    "            if autors[i] in p:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'Literature data/triplet_cache'\n",
    "train_dir = 'Literature data/train_set'\n",
    "test_dir = 'Literature data/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TripletModel(input_shape=(160, 160, 3), alpha = 1, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TripletModel at 0x18bdb4d0908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv1 due to mismatch in shape ((3, 3, 3, 24) vs (32, 3, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv1_bn due to mismatch in shape ((24,) vs (32,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_1 due to mismatch in shape ((3, 3, 24, 1) vs (3, 3, 32, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_1_bn due to mismatch in shape ((24,) vs (32,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_1 due to mismatch in shape ((1, 1, 24, 48) vs (64, 32, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_1_bn due to mismatch in shape ((48,) vs (64,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_2 due to mismatch in shape ((3, 3, 48, 1) vs (3, 3, 64, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_2_bn due to mismatch in shape ((48,) vs (64,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_2 due to mismatch in shape ((1, 1, 48, 96) vs (128, 64, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_2_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_3 due to mismatch in shape ((3, 3, 96, 1) vs (3, 3, 128, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_3_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_3 due to mismatch in shape ((1, 1, 96, 96) vs (128, 128, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_3_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_4 due to mismatch in shape ((3, 3, 96, 1) vs (3, 3, 128, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_4_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_4 due to mismatch in shape ((1, 1, 96, 192) vs (256, 128, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_4_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_5 due to mismatch in shape ((3, 3, 192, 1) vs (3, 3, 256, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_5_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_5 due to mismatch in shape ((1, 1, 192, 192) vs (256, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_5_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_6 due to mismatch in shape ((3, 3, 192, 1) vs (3, 3, 256, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_6_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_6 due to mismatch in shape ((1, 1, 192, 384) vs (512, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_6_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_7 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_7_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_7 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_7_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_8 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_8_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_8 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_8_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_9 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_9_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_9 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_9_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_10 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_10_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_10 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_10_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_11 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_11_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_11 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_11_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_12 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_12_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_12 due to mismatch in shape ((1, 1, 384, 768) vs (1024, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_12_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_13 due to mismatch in shape ((3, 3, 768, 1) vs (3, 3, 1024, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_13_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_13 due to mismatch in shape ((1, 1, 768, 768) vs (1024, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_13_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer dense_1 due to mismatch in shape ((768, 128) vs (1024, 128)).\n",
      "  weight_values[i].shape))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('Literature Data/classification_cache/final_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import plot\n",
    "def show(img):\n",
    "    \"\"\"\n",
    "    show rgb image\n",
    "    \"\"\"\n",
    "    ax = plt.axes([0,0,4,4], frameon=False)\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_csv = 'Literature data/train.csv'\n",
    "\n",
    "train = pd.read_csv(train_csv)\n",
    "# validation = pd.read_csv(validation_csv)\n",
    "x_train, y_train = train['file_name'].as_matrix(), train['label'].as_matrix()\n",
    "# x_validation, y_validation = validation['file_name'].as_matrix(), validation['label'].as_matrix()\n",
    "\n",
    "str2ind_train_dict, ind2str_train_dict = get_str2numb_numb2dict(y_train)\n",
    "y_train = np.array(apply_dict(str2ind_train_dict, y_train))\n",
    "\n",
    "# str2ind_val_dict, ind2str_val_dict = get_str2numb_numb2dict(y_validation)\n",
    "# y_validation = np.array(apply_dict(str2ind_val_dict, y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = model\n",
    "self.num_classes = len(np.unique(y_train))\n",
    "self.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WordsSequence at 0x26e1358e2b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "margin=0.5\n",
    "train_generator = WordsSequence(train_dir, input_shape=self.input_shape, x_set=x_train, y_set=y_train, batch_size=batch_size)\n",
    "# validation_generator = WordsSequence(validation_dir, input_shape=self.input_shape, x_set=validation_pairs, y_set=validation_y, batch_size=batch_size)\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# optimize = RMSprop(lr=learning_rate)\n",
    "optimize = Adam(lr=0.00001)\n",
    "self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.model.compile(loss=triplet_loss(margin=1.0, strategy=\"batch_all\"), optimizer=optimize)\n",
    "\n",
    "self.model.compile(loss=triplet_loss(margin=1.0, strategy=\"batch_semi_hard\"), optimizer=optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 923s 8s/step - loss: 0.9483\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 934s 8s/step - loss: 0.9417\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 887s 8s/step - loss: 0.9269\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 904s 8s/step - loss: 0.9363\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 947s 8s/step - loss: 0.9248\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 897s 8s/step - loss: 0.9122\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.9112\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 841s 7s/step - loss: 0.9074\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 829s 7s/step - loss: 0.9016\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 932s 8s/step - loss: 0.8888\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 911s 8s/step - loss: 0.8563\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 951s 8s/step - loss: 0.8786\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 929s 8s/step - loss: 0.8561\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 935s 8s/step - loss: 0.8480\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 932s 8s/step - loss: 0.8303\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 867s 7s/step - loss: 0.8451\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 842s 7s/step - loss: 0.8186\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 929s 8s/step - loss: 0.8392\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 934s 8s/step - loss: 0.8299\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 914s 8s/step - loss: 0.8051\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 966s 8s/step - loss: 0.7986\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 927s 8s/step - loss: 0.7947\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.8103\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7959\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 878s 8s/step - loss: 0.7778\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 874s 8s/step - loss: 0.7997\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.7788\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 879s 8s/step - loss: 0.7983\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 869s 7s/step - loss: 0.7896\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7624\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7647\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.7709\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 866s 7s/step - loss: 0.7743\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 859s 7s/step - loss: 0.7592\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 862s 7s/step - loss: 0.7681\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 873s 8s/step - loss: 0.7432\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 880s 8s/step - loss: 0.7502\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.7603\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.7323\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7393\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 855s 7s/step - loss: 0.7425\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7569\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7371\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.7517\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 871s 8s/step - loss: 0.7304\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 854s 7s/step - loss: 0.7268\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.7456\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7312\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7227\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7388\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 866s 7s/step - loss: 0.7044\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 865s 7s/step - loss: 0.7170\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.7081\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7088\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 841s 7s/step - loss: 0.7160\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 837s 7s/step - loss: 0.7017\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 845s 7s/step - loss: 0.7023\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 864s 7s/step - loss: 0.6904\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.6974\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.6905\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.7017\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.6938\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 859s 7s/step - loss: 0.7007\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 881s 8s/step - loss: 0.6785\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 854s 7s/step - loss: 0.6720\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.6797\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.6867\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.6730\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.6888\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 847s 7s/step - loss: 0.6659\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.6725\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 855s 7s/step - loss: 0.6765\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 877s 8s/step - loss: 0.6533\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.6498\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 840s 7s/step - loss: 0.6466\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.6562\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.6381\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 856s 7s/step - loss: 0.6404\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 890s 8s/step - loss: 0.6415\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 873s 8s/step - loss: 0.6413\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 865s 7s/step - loss: 0.6686\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 1083s 9s/step - loss: 0.6602\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 869s 7s/step - loss: 0.6530\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 826s 7s/step - loss: 0.6636\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 837s 7s/step - loss: 0.6244\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 819s 7s/step - loss: 0.6315\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.6491\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.6409\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 805s 7s/step - loss: 0.6263\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 810s 7s/step - loss: 0.6232\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.6082\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 806s 7s/step - loss: 0.6095\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 812s 7s/step - loss: 0.6073\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5897\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.5961\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.6157\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.6144\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 802s 7s/step - loss: 0.6098\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.6043\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 814s 7s/step - loss: 0.5937\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.5902\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.5734\n",
      "Epoch 103/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.5993\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 803s 7s/step - loss: 0.5787\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 810s 7s/step - loss: 0.5742\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 775s 7s/step - loss: 0.5727\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.5981\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5810\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5625\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.5930\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 803s 7s/step - loss: 0.5601\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.6001\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5627\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 807s 7s/step - loss: 0.5715\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5710\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.5493\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.5584\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5570\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.5416\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5345\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.5583\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.5584\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 806s 7s/step - loss: 0.5661\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.5558\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.5304\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5271\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.5199\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.5088\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.5156\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 804s 7s/step - loss: 0.5240\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5084\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5240\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.5145\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.5009\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.5285\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.5256\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 808s 7s/step - loss: 0.4942\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.5101\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 780s 7s/step - loss: 0.4926\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 775s 7s/step - loss: 0.5080\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.5083\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 780s 7s/step - loss: 0.4982\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 776s 7s/step - loss: 0.4858\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 778s 7s/step - loss: 0.4872\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 781s 7s/step - loss: 0.5040\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 778s 7s/step - loss: 0.4760\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 781s 7s/step - loss: 0.4931\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.4568\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 783s 7s/step - loss: 0.4800\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 812s 7s/step - loss: 0.4846\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4763\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.4652\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4844\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4705\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4607\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4699\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 807s 7s/step - loss: 0.4943\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.4797\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4730\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4546\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4608\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 785s 7s/step - loss: 0.4483\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.4360\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4494\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.4513\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4440\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4512\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 811s 7s/step - loss: 0.4222\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.4156\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4328\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4486\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 793s 7s/step - loss: 0.4599\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4394\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 782s 7s/step - loss: 0.4160\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 818s 7s/step - loss: 0.4389\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 805s 7s/step - loss: 0.4269\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4187\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.4038\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 823s 7s/step - loss: 0.4312\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.3996\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4247\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4012\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4203\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 824s 7s/step - loss: 0.4246\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.3972\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4068\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 815s 7s/step - loss: 0.3751\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.3941\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.3677\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 818s 7s/step - loss: 0.3782\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 819s 7s/step - loss: 0.3768\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.3765\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.4412\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 807s 7s/step - loss: 0.3910\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 809s 7s/step - loss: 0.3616\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.3842\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.3763\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 816s 7s/step - loss: 0.3821\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 792s 7s/step - loss: 0.3795\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.3943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26efbe7bf98>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_data=validation_generator, \n",
    "self.model.fit_generator(train_generator, shuffle=True, epochs=200, verbose=1, \n",
    "callbacks=[ModelCheckpoint(filepath=os.path.join(self.cache_dir, 'checkpoint-{epoch:02d}.h5'), save_weights_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'Literature Data/triplet_cache/my_final_model.h5'\n",
    "self.model.save(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_weights = self.cache_dir + '/' + 'my_final_weights.h5'\n",
    "self.save_weights(path_to_save_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = train_dir\n",
    "test_csv = 'Literature data/test.csv'\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeap\n"
     ]
    }
   ],
   "source": [
    "if self.embeddings is not None:\n",
    "    print(self.embeddings[0][0])\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "else:\n",
    "    print('Yeap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По частям делаю else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 178s 2s/step\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv)\n",
    "x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "\n",
    "self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "pred = self.model.predict_generator(words, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00762717e-01,  1.38824418e-01, -1.99312828e-02, ...,\n",
       "         3.62386629e-02,  6.29772060e-03,  1.07450590e-01],\n",
       "       [-8.49247426e-02,  1.20439470e-01, -2.18481780e-03, ...,\n",
       "         1.63447205e-02,  9.15719866e-05,  1.10471539e-01],\n",
       "       [-9.62019190e-02,  1.22878075e-01, -9.91626456e-03, ...,\n",
       "         1.99544169e-02,  9.16776992e-03,  1.09453216e-01],\n",
       "       ...,\n",
       "       [-1.09470241e-01,  1.35934681e-01, -1.08547136e-02, ...,\n",
       "         1.89207606e-02,  2.57770298e-03,  1.03421457e-01],\n",
       "       [-9.90877450e-02,  1.07763655e-01, -1.20304301e-02, ...,\n",
       "         9.89381410e-03,  1.19910911e-02,  1.27858549e-01],\n",
       "       [-1.02851026e-01,  1.21404469e-01, -1.88112240e-02, ...,\n",
       "         2.57086288e-02,  1.31449988e-02,  1.19343467e-01]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "self.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.clf.fit(pred, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.10076272, 0.13882442, -0.019931283, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [[-0.10076272, 0.13882442, -0.019931283, 0.003...\n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.save_embeddings('Literature Data/triplet_cache/embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вся функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_embeddings(train_dir, \"train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"../data/test.csv\", batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'Literature Data/train.csv'\n",
    "img_dir = 'Literature Data/train_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 281s 2s/step\n"
     ]
    }
   ],
   "source": [
    "if self.embeddings is not None:\n",
    "    print(self.embeddings[0][0])\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "else:\n",
    "    data = pd.read_csv(csv)\n",
    "    x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "\n",
    "    self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "    y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "    words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "    pred = self.model.predict_generator(words, verbose=1)\n",
    "\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(pred, y) \n",
    "\n",
    "    self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "    self.save_embeddings('embeddings.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "img_dir = test_dir\n",
    "self.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = 'Literature Data/test.csv'\n",
    "test = pd.read_csv(test_csv)\n",
    "x_test, y_test = test['file_name'].as_matrix(), test['label'].as_matrix()\n",
    "\n",
    "str2ind_test_dict, ind2str_test_dict = get_str2numb_numb2dict(y_test)\n",
    "test_y = np.array(apply_dict(str2ind_test_dict, y_test))\n",
    "\n",
    "words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x_test, batch_size=batch_size)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 221s 3s/step\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = self.model.predict_generator(words, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word accuracy:  0.19163636363636363\n"
     ]
    }
   ],
   "source": [
    "res = self.clf.predict(test_embeddings) \n",
    "predict = np.array(apply_dict(ind2str_test_dict , res))\n",
    "count = 0\n",
    "for i,j in zip(predict, y_test):\n",
    "    if i == j:\n",
    "        count += 1\n",
    "\n",
    "print('word accuracy: ', count / len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 autor accuracy:  0.3611111111111111\n",
      "top-5 autor accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "autors = np.unique(y_test)\n",
    "autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "    if p == autors[i]:\n",
    "        count += 1\n",
    "\n",
    "print('top-1 autor accuracy: ', count / len(autors))\n",
    "\n",
    "count = 0\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = [pair[0] for pair in Counter(np.ravel(predict[inds])).most_common(5)]\n",
    "    if autors[i] in p:\n",
    "        count += 1\n",
    "\n",
    "print('top-5 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не совпал автор №   1_0 не совпало с 1_20\n",
      "совпал автор №  1_1\n",
      "не совпал автор №   1_10 не совпало с 4_22\n",
      "не совпал автор №   1_12 не совпало с 3_13\n",
      "совпал автор №  1_13\n",
      "совпал автор №  1_14\n",
      "совпал автор №  1_15\n",
      "не совпал автор №   1_16 не совпало с 4_17\n",
      "совпал автор №  1_17\n",
      "совпал автор №  1_18\n",
      "совпал автор №  1_19\n",
      "не совпал автор №   1_2 не совпало с 1_22\n",
      "совпал автор №  1_20\n",
      "совпал автор №  1_21\n",
      "совпал автор №  1_22\n",
      "не совпал автор №   1_23 не совпало с 5_9\n",
      "совпал автор №  1_24\n",
      "не совпал автор №   1_3 не совпало с 4_20\n",
      "совпал автор №  1_4\n",
      "не совпал автор №   1_5 не совпало с 1_24\n",
      "совпал автор №  1_6\n",
      "совпал автор №  1_8\n",
      "совпал автор №  1_9\n",
      "совпал автор №  2_0\n",
      "совпал автор №  2_1\n",
      "не совпал автор №   2_10 не совпало с 5_16\n",
      "не совпал автор №   2_11 не совпало с 3_12\n",
      "не совпал автор №   2_12 не совпало с 6_12\n",
      "совпал автор №  2_13\n",
      "совпал автор №  2_14\n",
      "совпал автор №  2_15\n",
      "совпал автор №  2_2\n",
      "не совпал автор №   2_3 не совпало с 3_9\n",
      "совпал автор №  2_4\n",
      "совпал автор №  2_5\n",
      "совпал автор №  2_6\n",
      "не совпал автор №   2_8 не совпало с 1_10\n",
      "не совпал автор №   2_9 не совпало с 5_17\n",
      "совпал автор №  3_0\n",
      "совпал автор №  3_1\n",
      "совпал автор №  3_11\n",
      "совпал автор №  3_12\n",
      "совпал автор №  3_13\n",
      "не совпал автор №   3_14 не совпало с 5_0\n",
      "совпал автор №  3_15\n",
      "не совпал автор №   3_16 не совпало с 6_9\n",
      "совпал автор №  3_17\n",
      "совпал автор №  3_18\n",
      "совпал автор №  3_19\n",
      "не совпал автор №   3_2 не совпало с 4_3\n",
      "совпал автор №  3_3\n",
      "совпал автор №  3_4\n",
      "совпал автор №  3_5\n",
      "не совпал автор №   3_6 не совпало с 6_12\n",
      "совпал автор №  3_8\n",
      "совпал автор №  3_9\n",
      "не совпал автор №   4_1 не совпало с 4_20\n",
      "не совпал автор №   4_11 не совпало с 3_15\n",
      "не совпал автор №   4_15 не совпало с 4_17\n",
      "не совпал автор №   4_17 не совпало с 4_18\n",
      "не совпал автор №   4_18 не совпало с 4_19\n",
      "не совпал автор №   4_19 не совпало с 4_1\n",
      "не совпал автор №   4_2 не совпало с 4_3\n",
      "не совпал автор №   4_20 не совпало с 4_21\n",
      "не совпал автор №   4_21 не совпало с 3_12\n",
      "не совпал автор №   4_22 не совпало с 4_2\n",
      "не совпал автор №   4_3 не совпало с 4_4\n",
      "не совпал автор №   4_4 не совпало с 4_5\n",
      "не совпал автор №   4_5 не совпало с 4_6\n",
      "не совпал автор №   4_6 не совпало с 4_7\n",
      "не совпал автор №   4_7 не совпало с 3_15\n",
      "не совпал автор №   4_8 не совпало с 4_9\n",
      "не совпал автор №   4_9 не совпало с 5_0\n",
      "не совпал автор №   5_0 не совпало с 5_10\n",
      "не совпал автор №   5_1 не совпало с 5_2\n",
      "не совпал автор №   5_10 не совпало с 4_19\n",
      "не совпал автор №   5_11 не совпало с 5_12\n",
      "не совпал автор №   5_12 не совпало с 5_13\n",
      "не совпал автор №   5_13 не совпало с 5_14\n",
      "не совпал автор №   5_14 не совпало с 5_15\n",
      "не совпал автор №   5_15 не совпало с 5_16\n",
      "не совпал автор №   5_16 не совпало с 3_13\n",
      "не совпал автор №   5_17 не совпало с 4_9\n",
      "не совпал автор №   5_18 не совпало с 5_1\n",
      "не совпал автор №   5_2 не совпало с 5_9\n",
      "не совпал автор №   5_9 не совпало с 6_10\n",
      "не совпал автор №   6_0 не совпало с 6_11\n",
      "не совпал автор №   6_10 не совпало с 6_12\n",
      "не совпал автор №   6_11 не совпало с 6_6\n",
      "не совпал автор №   6_12 не совпало с 6_7\n",
      "не совпал автор №   6_6 не совпало с 6_8\n",
      "не совпал автор №   6_7 не совпало с 6_9\n",
      "не совпал автор №   6_8 не совпало с 7_0\n",
      "не совпал автор №   6_9 не совпало с 7_10\n",
      "не совпал автор №   7_0 не совпало с 7_11\n",
      "не совпал автор №   7_1 не совпало с 7_3\n",
      "не совпал автор №   7_10 не совпало с 7_12\n",
      "не совпал автор №   7_11 не совпало с 7_13\n",
      "не совпал автор №   7_12 не совпало с 7_1\n",
      "не совпал автор №   7_13 не совпало с 7_9\n",
      "не совпал автор №   7_2 не совпало с 7_4\n",
      "не совпал автор №   7_3 не совпало с 7_5\n",
      "не совпал автор №   7_4 не совпало с 7_6\n",
      "не совпал автор №   7_5 не совпало с 7_7\n",
      "не совпал автор №   7_6 не совпало с 7_3\n",
      "не совпал автор №   7_7 не совпало с 7_8\n",
      "не совпал автор №   7_8 не совпало с 7_3\n",
      "совпал автор №  7_9\n",
      "top-1 autor accuracy:  0.3611111111111111\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "autors = np.unique(y_test)\n",
    "autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "    if p == autors[i]:\n",
    "        print('совпал автор №  ' + str(autors[i]))\n",
    "        count += 1\n",
    "    else:\n",
    "         print('не совпал автор №   ' + str(autors[i]) + ' не совпало с ' + str(p))\n",
    "\n",
    "print('top-1 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не совпал автор №   1_0 не совпало с [11, 0, 43, 89, 9]\n",
      "не совпал автор №   1_1 не совпало с [10, 36, 29, 55, 80]\n",
      "не совпал автор №   1_10 не совпало с [64, 1, 105, 38, 107]\n",
      "не совпал автор №   1_12 не совпало с [41, 60, 2, 93, 8]\n",
      "не совпал автор №   1_13 не совпало с [3, 89, 14, 20, 52]\n",
      "не совпал автор №   1_14 не совпало с [4, 33, 5, 44, 52]\n",
      "не совпал автор №   1_15 не совпало с [5, 34, 4, 97, 6]\n",
      "не совпал автор №   1_16 не совпало с [58, 34, 47, 91, 73]\n",
      "не совпал автор №   1_17 не совпало с [7, 54, 45, 26, 59]\n",
      "не совпал автор №   1_18 не совпало с [8, 38, 73, 88, 71]\n",
      "не совпал автор №   1_19 не совпало с [9, 79, 11, 57, 0]\n",
      "не совпал автор №   1_2 не совпало с [13, 12, 92, 80, 71]\n",
      "не совпал автор №   1_20 не совпало с [11, 3, 19, 24, 89]\n",
      "не совпал автор №   1_21 не совпало с [12, 39, 1, 22, 33]\n",
      "не совпал автор №   1_22 не совпало с [13, 79, 43, 3]\n",
      "не совпал автор №   1_23 не совпало с [85, 14, 45, 20, 13]\n",
      "не совпал автор №   1_24 не совпало с [15, 46, 31, 11, 61]\n",
      "не совпал автор №   1_3 не совпало с [62, 76, 51, 17, 32]\n",
      "не совпал автор №   1_4 не совпало с [18, 54]\n",
      "не совпал автор №   1_5 не совпало с [15, 46, 20, 104, 92]\n",
      "не совпал автор №   1_6 не совпало с [20, 85, 72, 46, 8]\n",
      "не совпал автор №   1_8 не совпало с [21, 38, 77, 39, 40]\n",
      "не совпал автор №   1_9 не совпало с [22, 42, 41, 58, 75]\n",
      "не совпал автор №   2_0 не совпало с [23, 93, 94, 43, 24]\n",
      "не совпал автор №   2_1 не совпало с [30, 58, 48, 88, 50]\n",
      "не совпал автор №   2_10 не совпало с [80, 69, 24, 58, 16]\n",
      "не совпал автор №   2_11 не совпало с [40, 106, 88, 96, 25]\n",
      "не совпал автор №   2_12 не совпало с [89, 26, 3, 79, 43]\n",
      "не совпал автор №   2_13 не совпало с [27, 40, 74, 63, 19]\n",
      "не совпал автор №   2_14 не совпало с [28, 69, 84, 15, 47]\n",
      "не совпал автор №   2_15 не совпало с [29, 68, 47, 46, 102]\n",
      "не совпал автор №   2_2 не совпало с [31, 62, 53, 32, 40]\n",
      "не совпал автор №   2_3 не совпало с [55, 32, 53, 58, 17]\n",
      "не совпал автор №   2_4 не совпало с [33, 87, 34, 41, 58]\n",
      "не совпал автор №   2_5 не совпало с [34, 5, 6, 31, 52]\n",
      "не совпал автор №   2_6 не совпало с [35, 6, 53, 48, 4]\n",
      "не совпал автор №   2_8 не совпало с [1, 36, 73, 76, 40]\n",
      "не совпал автор №   2_9 не совпало с [81, 37, 40, 72, 65]\n",
      "не совпал автор №   3_0 не совпало с [38, 8, 60, 40, 3]\n",
      "не совпал автор №   3_1 не совпало с [48, 62, 40, 37, 88]\n",
      "не совпал автор №   3_11 не совпало с [39, 38, 60, 77, 93]\n",
      "не совпал автор №   3_12 не совпало с [40, 88, 38, 60, 43]\n",
      "не совпал автор №   3_13 не совпало с [41, 39, 97, 8, 12]\n",
      "не совпал автор №   3_14 не совпало с [73, 42, 22, 8, 64]\n",
      "не совпал автор №   3_15 не совпало с [43, 11, 13, 79, 41]\n",
      "не совпал автор №   3_16 не совпало с [93, 44, 11, 23, 80]\n",
      "не совпал автор №   3_17 не совпало с [45, 83, 0, 38, 3]\n",
      "не совпал автор №   3_18 не совпало с [46, 84, 91, 28, 90]\n",
      "не совпал автор №   3_19 не совпало с [47, 90, 91, 66, 46]\n",
      "не совпал автор №   3_2 не совпало с [66, 49, 83, 90, 47]\n",
      "не совпал автор №   3_3 не совпало с [50, 29, 88, 30]\n",
      "не совпал автор №   3_4 не совпало с [51, 17, 84, 42, 69]\n",
      "не совпал автор №   3_5 не совпало с [52, 4, 33, 36, 31]\n",
      "не совпал автор №   3_6 не совпало с [89, 3, 9, 7, 93]\n",
      "не совпал автор №   3_8 не совпало с [54, 7, 87, 32, 33]\n",
      "не совпал автор №   3_9 не совпало с [55, 4, 53, 52, 15]\n",
      "не совпал автор №   4_1 не совпало с [62, 55, 40, 88, 81]\n",
      "не совпал автор №   4_11 не совпало с [43, 92, 13, 78, 57]\n",
      "не совпал автор №   4_15 не совпало с [58, 34, 33, 6, 21]\n",
      "не совпал автор №   4_17 не совпало с [59, 95, 93, 27, 56]\n",
      "не совпал автор №   4_18 не совпало с [60, 39, 38, 41, 40]\n",
      "не совпал автор №   4_19 не совпало с [61, 20, 92, 37, 42]\n",
      "не совпал автор №   4_2 не совпало с [66, 49, 83, 90, 47]\n",
      "не совпал автор №   4_20 не совпало с [63, 51, 30, 15, 19]\n",
      "не совпал автор №   4_21 не совпало с [40, 64, 101, 33, 88]\n",
      "не совпал автор №   4_22 не совпало с [65, 17, 95, 37, 28]\n",
      "не совпал автор №   4_3 не совпало с [67, 48, 35, 30, 84]\n",
      "не совпал автор №   4_4 не совпало с [68, 90, 88, 29, 91]\n",
      "не совпал автор №   4_5 не совпало с [69, 24, 10, 82, 29]\n",
      "не совпал автор №   4_6 не совпало с [70, 77, 17, 21, 73]\n",
      "не совпал автор №   4_7 не совпало с [43, 3, 80, 42, 65]\n",
      "не совпал автор №   4_8 не совпало с [72, 82, 40, 69, 92]\n",
      "не совпал автор №   4_9 не совпало с [73, 34, 102, 64, 17]\n",
      "не совпал автор №   5_0 не совпало с [74, 94, 13, 65, 98]\n",
      "не совпал автор №   5_1 не совпало с [84, 69, 85, 28, 46]\n",
      "не совпал автор №   5_10 не совпало с [60, 77, 75, 98, 22]\n",
      "не совпал автор №   5_11 не совпало с [76, 74, 65, 17, 47]\n",
      "не совпал автор №   5_12 не совпало с [77, 70, 12, 36, 48]\n",
      "не совпал автор №   5_13 не совпало с [78, 79, 43, 57, 18]\n",
      "не совпал автор №   5_14 не совпало с [79, 3, 13, 10, 11]\n",
      "не совпал автор №   5_15 не совпало с [80, 38, 21, 77, 60]\n",
      "не совпал автор №   5_16 не совпало с [41, 82, 40, 81, 72]\n",
      "не совпал автор №   5_17 не совпало с [72, 82, 43, 98, 69]\n",
      "не совпал автор №   5_18 не совпало с [83, 0, 11, 90, 50]\n",
      "не совпал автор №   5_2 не совпало с [85, 29, 11, 15, 102]\n",
      "не совпал автор №   5_9 не совпало с [87, 39, 60, 41, 3]\n",
      "не совпал автор №   6_0 не совпало с [88, 40, 30, 50, 33]\n",
      "не совпал автор №   6_10 не совпало с [89, 11, 93, 9, 77]\n",
      "не совпал автор №   6_11 не совпало с [90, 66, 81, 29, 49]\n",
      "не совпал автор №   6_12 не совпало с [91, 47, 15, 90, 84]\n",
      "не совпал автор №   6_6 не совпало с [92, 61, 74, 82, 3]\n",
      "не совпал автор №   6_7 не совпало с [93, 8, 43, 25, 92]\n",
      "не совпал автор №   6_8 не совпало с [94, 79, 14, 82, 80]\n",
      "не совпал автор №   6_9 не совпало с [95, 59, 51, 78, 56]\n",
      "не совпал автор №   7_0 не совпало с [96, 103, 98, 101, 97]\n",
      "не совпал автор №   7_1 не совпало с [101, 68, 104, 107, 30]\n",
      "не совпал автор №   7_10 не совпало с [97, 98, 106, 93, 73]\n",
      "не совпал автор №   7_11 не совпало с [98, 103, 106, 90, 5]\n",
      "не совпал автор №   7_12 не совпало с [99, 100, 104]\n",
      "не совпал автор №   7_13 не совпало с [107, 105, 101, 100, 88]\n",
      "не совпал автор №   7_2 не совпало с [102, 20, 50, 37, 106]\n",
      "не совпал автор №   7_3 не совпало с [103, 100, 104]\n",
      "не совпал автор №   7_4 не совпало с [104, 68, 0]\n",
      "не совпал автор №   7_5 не совпало с [105, 107, 83]\n",
      "не совпал автор №   7_6 не совпало с [101, 105, 96, 1, 107]\n",
      "не совпал автор №   7_7 не совпало с [106, 101, 107, 91]\n",
      "не совпал автор №   7_8 не совпало с [101, 98, 105, 106, 36]\n",
      "не совпал автор №   7_9 не совпало с [107, 105, 34, 101]\n",
      "top-5 autor accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = [pair[0] for pair in Counter(np.ravel(res[inds])).most_common(5)]\n",
    "    if autors[i] in p:\n",
    "        print('совпал автор №  ' + str(autors[i]))\n",
    "        count += 1\n",
    "    else:\n",
    "        print('не совпал автор №   ' + str(autors[i]) + ' не совпало с ' + str(p))\n",
    "\n",
    "print('top-5 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triplet_model import TripletModel\n",
    "\n",
    "train_dir = 'C:/Users/Anastasia/Pictures/words_train'\n",
    "validation_dir = 'C:/Users/Anastasia/Pictures/words_validation'\n",
    "test_dir = '../data/words_test'\n",
    "\n",
    "# Train\n",
    "# model = TripletModel(input_shape=(160, 160, 3), cache_dir=\"triplet_cache_new\")\n",
    "# model.load_weights(\"triplet_cache_new/train_all/checkpoint-06.h5\")\n",
    "# model.train(train_dir, \"train.csv\", validation_dir, \"validation.csv\", epochs=200)\n",
    " \n",
    "# Predict\n",
    "model = TripletModel(alpha=0.75, input_shape=(160, 160, 3), cache_dir=\"triplet_cache\")\n",
    "model.load_weights(\"final_weigths_alpha_0.75/final.h5\")\n",
    "model.load_embeddings('../data/triplet_embeddings_75.pkl')\n",
    "model.make_embeddings(train_dir, \"train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"../data/test.csv\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 24)        216       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 48)        1152      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 96)        4608      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 192)       18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 384)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 384)         3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 768)         294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 1,931,408\n",
      "Trainable params: 1,914,992\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08118027 -0.00346136  0.07533943 ...  0.01582423  0.14860386\n",
      "  -0.05757323]\n",
      " [ 0.06136449  0.04853927  0.046064   ...  0.00425277  0.06847595\n",
      "   0.04984679]\n",
      " [ 0.09801209  0.02759981  0.05831361 ...  0.07109818  0.11770546\n",
      "  -0.01831287]\n",
      " ...\n",
      " [ 0.04524183 -0.02268627  0.08047596 ...  0.1392696   0.14579052\n",
      "   0.0180465 ]\n",
      " [ 0.02804308  0.01359361  0.11527269 ...  0.10519566  0.11367899\n",
      "   0.03562412]\n",
      " [ 0.09554158  0.01088285  0.0726615  ...  0.1196068   0.14568764\n",
      "   0.03558488]]\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 24)        216       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 48)        1152      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 96)        4608      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 192)       18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 384)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 384)         3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 768)         294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 1,931,408\n",
      "Trainable params: 1,914,992\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:160: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607/1607 [==============================] - 61s 38ms/step\n",
      "word accuracy:  0.0043559427504667085\n",
      "top-5 autor accuracy:  0.0\n",
      "top-5 autor accuracy:  0.042105263157894736\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'writer_identification-master/data/words_test'\n",
    "model = TripletModel(alpha=0.75, input_shape=(160, 160, 3), cache_dir=\"triplet_cache\")\n",
    "model.load_weights('final.h5')\n",
    "model.load_embeddings('writer_identification-master/data/triplet_embeddings_75.pkl')\n",
    "model.make_embeddings(train_dir, \"writer_identification-master/data/train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"writer_identification-master/data/test.csv\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
