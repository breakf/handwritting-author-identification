{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def valid_triplets_mask(labels):\n",
    "    \"\"\"Compute the 3D boolean mask where mask[a, p, n] is True if (a, p, n) is a valid triplet,\n",
    "    as in a, p, n are distinct and labels[a] == labels[p], labels[a] != labels[n].\n",
    "\n",
    "    :param labels: tensor of shape (batch_size,)\n",
    "    :return mask: tf.bool tensor of shape (batch_size, batch_size, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def euclidean_distance(embeddings, squared=False):\n",
    "    \"\"\"Computes pairwise euclidean distance matrix with numerical stability.\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    :param embeddings: 2-D Tensor of size [number of data, feature dimension].\n",
    "    :param squared: Boolean, whether or not to square the pairwise distances.\n",
    "    :return dist: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    dist_squared = tf.add(tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True),\n",
    "                          tf.reduce_sum(tf.square(tf.transpose(embeddings)), axis=0, keepdims=True)\n",
    "                          ) - 2.0 * tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    dist_squared = tf.maximum(dist_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = tf.less_equal(dist_squared, 0.0)\n",
    "    # Optionally take the sqrt.\n",
    "    dist = dist_squared if squared else tf.sqrt(dist_squared + tf.cast(error_mask, dtype=tf.float32) * 1e-16)\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    dist = tf.multiply(dist, tf.cast(tf.logical_not(error_mask), dtype=tf.float32))\n",
    "\n",
    "    n_data = tf.shape(embeddings)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = tf.ones_like(dist) - tf.linalg.diag(tf.ones([n_data]))\n",
    "    dist = tf.multiply(dist, mask_offdiagonals)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the maximum.\n",
    "    :return masked_maximums: N-D `Tensor`. The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = tf.reduce_min(data, axis=dim, keepdims=True)\n",
    "    masked_maximums = tf.reduce_max(tf.multiply(data - axis_minimums, mask), axis=dim, keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "    :param data: 2-D float `Tensor` of size [n, m].\n",
    "    :param mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "    :param dim: The dimension over which to compute the minimum.\n",
    "    :return masked_minimums: N-D `Tensor`. The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = tf.reduce_max(data, axis=dim, keepdims=True)\n",
    "    masked_minimums = tf.reduce_min(tf.multiply(data - axis_maximums, mask), axis=dim, keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "\n",
    "\n",
    "def triplet_loss(margin=1.0, strategy='batch_semi_hard'):\n",
    "    \"\"\"Compute the triplet loss over the batch of embeddings. tf contrib inspired:\n",
    "    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/losses/python/metric_learning/metric_loss_ops.py\n",
    "\n",
    "    :param margin: margin that is going to be enforced by the triplet loss\n",
    "    :param strategy: string, that indicated whether we're using the 'batch hard', 'batch all' or 'batch_semi_hard' mining strategy\n",
    "    :return: a callback function that calculates the loss according to the specified strategy\n",
    "    \"\"\"\n",
    "    def get_loss_tensor(positive_dists, negative_dists):\n",
    "        \"\"\"Compute the triplet loss function tensor using specified margin:\n",
    "\n",
    "        :param positive_dists: positive distances tensor\n",
    "        :param negative_dists:  negative distances tensor\n",
    "        :return: resulting triplet loss tensor\n",
    "        \"\"\"\n",
    "        if margin == 'soft':\n",
    "            return tf.nn.softplus(positive_dists - negative_dists)\n",
    "\n",
    "        return tf.maximum(positive_dists - negative_dists + margin, 0.0)\n",
    "\n",
    "    def batch_semi_hard(labels, embeddings):\n",
    "        \"\"\"Computes the triplet loss with semi-hard negative mining.\n",
    "        The loss encourages the positive distances (between a pair of embeddings with\n",
    "        the same labels) to be smaller than the minimum negative distance among\n",
    "        which are at least greater than the positive distance plus the margin constant\n",
    "        (called semi-hard negative) in the mini-batch. If no such negative exists,\n",
    "        uses the largest negative distance instead.\n",
    "        See: https://arxiv.org/abs/1503.03832.\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        batch_size = tf.size(labels)\n",
    "        # Build pairwise squared distance matrix.\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        # Build pairwise binary adjacency matrix (equal label mask).\n",
    "        adjacency = tf.equal(labels, tf.transpose(labels))\n",
    "        # Invert so we can select negatives only.\n",
    "        adjacency_not = tf.logical_not(adjacency)\n",
    "\n",
    "        # Compute the mask.\n",
    "        dist_tile = tf.tile(dist, [batch_size, 1])  # stack dist matrix batch_size times, axis=0\n",
    "        mask = tf.logical_and(tf.tile(adjacency_not, [batch_size, 1]), tf.greater(dist_tile, tf.reshape(dist, [-1, 1])))\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        is_negatives_outside = tf.reshape(tf.greater(tf.reduce_sum(mask, axis=1, keepdims=True), 0.0), [batch_size, batch_size])\n",
    "        is_negatives_outside = tf.transpose(is_negatives_outside)\n",
    "\n",
    "        # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "        negatives_outside = tf.reshape(masked_minimum(dist_tile, mask), [batch_size, batch_size])\n",
    "        negatives_outside = tf.transpose(negatives_outside)\n",
    "\n",
    "        # negatives_inside: largest D_an.\n",
    "        adjacency_not = tf.cast(adjacency_not, dtype=tf.float32)\n",
    "        negatives_inside = tf.tile(masked_maximum(dist, adjacency_not), [1, batch_size])\n",
    "\n",
    "        semi_hard_negatives = tf.where(is_negatives_outside, negatives_outside, negatives_inside)\n",
    "\n",
    "        # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "        #   in semihard, they take all positive pairs except the diagonal.\n",
    "        mask_positives = tf.cast(adjacency, dtype=tf.float32) - tf.linalg.diag(tf.ones([batch_size]))\n",
    "        n_positives = tf.reduce_sum(mask_positives)\n",
    "\n",
    "        loss_mat = get_loss_tensor(dist, semi_hard_negatives)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(tf.multiply(loss_mat, mask_positives)), n_positives)\n",
    "        return loss\n",
    "\n",
    "    def batch_all(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating all the valid triplets and averaging over the positive ones\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        #mask = tf.to_float(valid_triplets_mask(labels))\n",
    "        mask = tf.cast(valid_triplets_mask(labels), dtype=tf.float32)\n",
    "\n",
    "        anchor_positive_dist = tf.expand_dims(dist, 2)\n",
    "        anchor_negative_dist = tf.expand_dims(dist, 1)\n",
    "\n",
    "        loss_tensor = get_loss_tensor(anchor_positive_dist, anchor_negative_dist)\n",
    "        loss_tensor = tf.multiply(loss_tensor, mask)\n",
    "\n",
    "        #num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_tensor, 1e-16)))\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.cast(tf.greater(loss_tensor, 1e-16), dtype=tf.float32))\n",
    "        #loss = tf.div_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        loss = tf.math.divide_no_nan(tf.reduce_sum(loss_tensor), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    def batch_hard(labels, embeddings):\n",
    "        \"\"\"Compute the loss by generating only hardest valid triplets and averaging over the positive ones.\n",
    "        One triplet per embedding, i.e. per anchor\n",
    "\n",
    "        :param labels: 1-D tf.int32 `Tensor` with shape [batch_size] of multiclass integer labels.\n",
    "        :param embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should be l2 normalized.\n",
    "        :return loss: tf.float32 scalar.\n",
    "        \"\"\"\n",
    "        dist = euclidean_distance(embeddings, squared=True)\n",
    "        adjacency = tf.cast(tf.equal(tf.reshape(labels, (-1, 1)), tf.reshape(labels, (1, -1))), tf.float32)\n",
    "\n",
    "        pos_dist = tf.reduce_max(adjacency * dist, axis=1)\n",
    "        inf = tf.constant(1e+9, tf.float32)\n",
    "        neg_dist = tf.reduce_min((adjacency * inf) + dist, axis=1)\n",
    "\n",
    "        loss_mat = get_loss_tensor(pos_dist, neg_dist)\n",
    "\n",
    "        num_non_easy_triplets = tf.reduce_sum(tf.to_float(tf.greater(loss_mat, 1e-16)))\n",
    "        loss = tf.div_no_nan(tf.reduce_sum(loss_mat), num_non_easy_triplets)\n",
    "        return loss\n",
    "\n",
    "    if strategy == 'batch_semi_hard':\n",
    "        return batch_semi_hard\n",
    "    elif strategy == 'batch hard':\n",
    "        return batch_hard\n",
    "    else:\n",
    "        return batch_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def fetch(img_dir, name):\n",
    "    #print('image ' + str(name))\n",
    "    img = cv2.imread(join(img_dir, name))\n",
    "    if img.shape == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    elif img.shape == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize(img, size=(1024, 768)):\n",
    "    assert len(size) == 2\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def pad(img, size=(1024, 768)):\n",
    "    assert len(img.shape) == 3\n",
    "    assert len(size) == 2\n",
    "    h, w, _ = img.shape\n",
    "    #assert w <= size[0] and h <= size[1]\n",
    "    pad_vert = np.ceil((size[1]-h) / 2).astype(np.uint32)\n",
    "    pad_hor = np.ceil((size[0]-w) / 2).astype(np.uint32)\n",
    "\n",
    "    padded = np.full((size[1], size[0], 3), 255).astype(np.uint8)\n",
    "    padded[pad_vert:pad_vert+h, pad_hor:pad_hor+w, :] = img.copy()\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "class WordsSequence(Sequence):\n",
    "    def __init__(self, img_dir, input_shape, x_set, y_set=None, batch_size=16):\n",
    "        if y_set is not None:\n",
    "            self.x, self.y = x_set, y_set\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "        else:\n",
    "            self.x, self.y = x_set, None\n",
    "            \n",
    "        self.img_dir = img_dir\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x])\n",
    "\n",
    "        unused = self.dataset.loc[self.dataset['used'] == 0]\n",
    "            \n",
    "        if len(unused) >= self.batch_size:\n",
    "            batch_indices = unused.sample(n=self.batch_size).index\n",
    "        else:\n",
    "            batch_indices = unused.sample(n=self.batch_size, replace=True).index\n",
    "\n",
    "        self.dataset.loc[batch_indices, 'used'] = 1\n",
    "        batch_x = self.dataset.iloc[batch_indices]['x'].values\n",
    "        batch_y = self.dataset.iloc[batch_indices]['y'].values\n",
    "        return np.array([self.preprocess(fetch(self.img_dir, name)) for name in batch_x]), np.array(batch_y)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        assert len(img.shape) == 3\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        if h / w <= self.input_shape[0] / self.input_shape[1]:\n",
    "            img = resize(img, (self.input_shape[1], int(self.input_shape[1] * h / w)))\n",
    "        else:\n",
    "            img = resize(img, (int(self.input_shape[0] * w / h), self.input_shape[0]))\n",
    "\n",
    "        img = pad(img, (self.input_shape[1], self.input_shape[0]))\n",
    "        return img / 255.  \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.y is not None:\n",
    "            self.dataset = pd.DataFrame(data={'x': self.x, 'y': self.y, 'used': np.zeros_like(self.y)})\n",
    "            self.dataset['class_count'] = self.dataset.groupby('y')['y'].transform('count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import  sample\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "\n",
    "\n",
    "def get_str2numb_numb2dict(vect):\n",
    "    str_to_ind_dict = {}\n",
    "    count = 0\n",
    "    for v in vect:\n",
    "        if v not in str_to_ind_dict.keys():\n",
    "            str_to_ind_dict[v] = count\n",
    "            count += 1\n",
    "    reverse_dict = {v:k for k, v in str_to_ind_dict.items()}\n",
    "    return str_to_ind_dict, reverse_dict\n",
    "\n",
    "def apply_dict(dict_keys, X):\n",
    "    res = []\n",
    "    for x in X:\n",
    "        res.append(dict_keys[x])\n",
    "    return res\n",
    "\n",
    "class ProgbarLossLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super(ProgbarLossLogger, self).__init__()\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.seen = 0\n",
    "        self.target = self.params['steps']\n",
    "\n",
    "        if self.epochs > 1:\n",
    "            print('Epoch %d/%d' % (epoch + 1, self.epochs))\n",
    "        self.progbar = Progbar(target=self.target, verbose=True, stateful_metrics=['loss'])\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.seen < self.target:\n",
    "            self.log_values = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        num_steps = logs.get('num_steps', 1)\n",
    "        self.seen += num_steps\n",
    "\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.log_values.append((k, logs[k]))\n",
    "        self.progbar.update(self.seen, self.log_values)\n",
    "        \n",
    "class TripletModel:\n",
    "    def __init__(self, alpha, input_shape, cache_dir):\n",
    "        self.alpha = alpha\n",
    "        self.input_shape = input_shape\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.isdir(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self.model = self.build_model()\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        base_network = MobileNet(input_shape=self.input_shape, alpha=self.alpha, weights='imagenet', include_top=False, \n",
    "                                 pooling='avg')\n",
    "        x = Dense(128)(base_network.output)\n",
    "        x = Lambda(lambda x: K.l2_normalize(x, axis=1))(x)\n",
    "        model = Model(inputs=base_network.input, outputs=x)\n",
    "        model.summary()\n",
    "        return model\n",
    "           \n",
    "    def train(self, train_dir, train_csv, validation_dir, validation_csv, epochs, batch_size=32, learning_rate=0.001, margin=0.5):\n",
    "        train = pd.read_csv(train_csv)\n",
    "        # validation = pd.read_csv(validation_csv)\n",
    "        x_train, y_train = train['file_name'].as_matrix(), train['label'].as_matrix()\n",
    "        # x_validation, y_validation = validation['file_name'].as_matrix(), validation['label'].as_matrix()\n",
    "        \n",
    "        str2ind_train_dict, ind2str_train_dict = get_str2numb_numb2dict(y_train)\n",
    "        y_train = np.array(apply_dict(str2ind_train_dict, y_train))\n",
    "\n",
    "        # str2ind_val_dict, ind2str_val_dict = get_str2numb_numb2dict(y_validation)\n",
    "        # y_validation = np.array(apply_dict(str2ind_val_dict, y_validation))\n",
    "        \n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        train_generator = WordsSequence(train_dir, input_shape=self.input_shape, x_set=x_train, y_set=y_train, batch_size=batch_size)\n",
    "        # validation_generator = WordsSequence(validation_dir, input_shape=self.input_shape, x_set=validation_pairs, y_set=validation_y, batch_size=batch_size)\n",
    "\n",
    "        # optimize = RMSprop(lr=learning_rate)\n",
    "        optimize = Adam(lr=0.00001)\n",
    "        self.model.summary()\n",
    "        self.model.compile(loss=triplet_loss.triplet_loss(margin=1.0, strategy=\"batch_all\"), optimizer=optimize)\n",
    "        \n",
    "        # validation_data=validation_generator, \n",
    "        self.model.fit_generator(train_generator, shuffle=True, epochs=epochs, verbose=1, \n",
    "        callbacks=[ModelCheckpoint(filepath=os.path.join(self.cache_dir, 'checkpoint-{epoch:02d}.h5'), save_weights_only=True)])\n",
    "        \n",
    "        self.model.save('final_model.h5')\n",
    "        self.save_weights('final_weights.h5')\n",
    "\n",
    "\n",
    "    def save_embeddings(self, filename):\n",
    "        self.embeddings.to_pickle(filename)\n",
    "    \n",
    "    def load_embeddings(self, filename):\n",
    "        self.embeddings = pd.read_pickle(filename)    \n",
    "        \n",
    "    def save_weights(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        self.model.load_weights(filename, by_name=True, skip_mismatch=True)\n",
    "        \n",
    "    \n",
    "    def make_embeddings(self, img_dir, csv, batch_size=32):\n",
    "        if self.embeddings is not None:\n",
    "            print(self.embeddings[0][0])\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "        else:\n",
    "            data = pd.read_csv(csv)\n",
    "            x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "            \n",
    "            self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "            y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "            words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "            pred = self.model.predict_generator(words, verbose=1)\n",
    "\n",
    "            self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "            self.clf.fit(pred, y) \n",
    "     \n",
    "            self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "            self.save_embeddings('embeddings.pkl')\n",
    "    \n",
    "    def predict(self, img_dir, test_csv, batch_size=32):\n",
    "        self.model.summary()\n",
    "        test = pd.read_csv(test_csv)\n",
    "        x_test, y_test = test['file_name'].as_matrix(), test['label'].as_matrix()\n",
    "        \n",
    "        str2ind_test_dict, ind2str_test_dict = get_str2numb_numb2dict(y_test)\n",
    "        test_y = np.array(apply_dict(str2ind_test_dict, y_test))\n",
    "\n",
    "        words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x_test, batch_size=batch_size)\n",
    "        test_embeddings = self.model.predict_generator(words, verbose=1)\n",
    " \n",
    "        res = self.clf.predict(test_embeddings) \n",
    "        predict = np.array(apply_dict(ind2str_test_dict , res))\n",
    "        count = 0\n",
    "        for i,j in zip(predict, y_test):\n",
    "            if i == j:\n",
    "                count += 1\n",
    "\n",
    "        print('word accuracy: ', count / len(y_test))\n",
    "        \n",
    "        count = 0\n",
    "        autors = np.unique(y_test)\n",
    "        autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "            if p == autors[i]:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    "        \n",
    "        count = 0\n",
    "        for i,inds in enumerate(autor_ind):\n",
    "            p = [pair[0] for pair in Counter(np.ravel(predict[inds])).most_common(5)]\n",
    "            if autors[i] in p:\n",
    "                count += 1\n",
    "\n",
    "        print('top-5 autor accuracy: ', count / len(autors))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'Literature data/triplet_cache'\n",
    "train_dir = 'Literature data/train_set'\n",
    "test_dir = 'Literature data/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TripletModel(input_shape=(160, 160, 3), alpha = 1, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TripletModel at 0x18bdb4d0908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv1 due to mismatch in shape ((3, 3, 3, 24) vs (32, 3, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv1_bn due to mismatch in shape ((24,) vs (32,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_1 due to mismatch in shape ((3, 3, 24, 1) vs (3, 3, 32, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_1_bn due to mismatch in shape ((24,) vs (32,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_1 due to mismatch in shape ((1, 1, 24, 48) vs (64, 32, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_1_bn due to mismatch in shape ((48,) vs (64,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_2 due to mismatch in shape ((3, 3, 48, 1) vs (3, 3, 64, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_2_bn due to mismatch in shape ((48,) vs (64,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_2 due to mismatch in shape ((1, 1, 48, 96) vs (128, 64, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_2_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_3 due to mismatch in shape ((3, 3, 96, 1) vs (3, 3, 128, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_3_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_3 due to mismatch in shape ((1, 1, 96, 96) vs (128, 128, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_3_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_4 due to mismatch in shape ((3, 3, 96, 1) vs (3, 3, 128, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_4_bn due to mismatch in shape ((96,) vs (128,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_4 due to mismatch in shape ((1, 1, 96, 192) vs (256, 128, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_4_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_5 due to mismatch in shape ((3, 3, 192, 1) vs (3, 3, 256, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_5_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_5 due to mismatch in shape ((1, 1, 192, 192) vs (256, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_5_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_6 due to mismatch in shape ((3, 3, 192, 1) vs (3, 3, 256, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_6_bn due to mismatch in shape ((192,) vs (256,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_6 due to mismatch in shape ((1, 1, 192, 384) vs (512, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_6_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_7 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_7_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_7 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_7_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_8 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_8_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_8 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_8_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_9 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_9_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_9 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_9_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_10 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_10_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_10 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_10_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_11 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_11_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_11 due to mismatch in shape ((1, 1, 384, 384) vs (512, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_11_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_12 due to mismatch in shape ((3, 3, 384, 1) vs (3, 3, 512, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_12_bn due to mismatch in shape ((384,) vs (512,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_12 due to mismatch in shape ((1, 1, 384, 768) vs (1024, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_12_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_13 due to mismatch in shape ((3, 3, 768, 1) vs (3, 3, 1024, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_dw_13_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_13 due to mismatch in shape ((1, 1, 768, 768) vs (1024, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer conv_pw_13_bn due to mismatch in shape ((768,) vs (1024,)).\n",
      "  weight_values[i].shape))\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\saving.py:1319: UserWarning: Skipping loading of weights for layer dense_1 due to mismatch in shape ((768, 128) vs (1024, 128)).\n",
      "  weight_values[i].shape))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('Literature Data/classification_cache/final_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import plot\n",
    "def show(img):\n",
    "    \"\"\"\n",
    "    show rgb image\n",
    "    \"\"\"\n",
    "    ax = plt.axes([0,0,4,4], frameon=False)\n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_csv = 'Literature data/train.csv'\n",
    "\n",
    "train = pd.read_csv(train_csv)\n",
    "# validation = pd.read_csv(validation_csv)\n",
    "x_train, y_train = train['file_name'].as_matrix(), train['label'].as_matrix()\n",
    "# x_validation, y_validation = validation['file_name'].as_matrix(), validation['label'].as_matrix()\n",
    "\n",
    "str2ind_train_dict, ind2str_train_dict = get_str2numb_numb2dict(y_train)\n",
    "y_train = np.array(apply_dict(str2ind_train_dict, y_train))\n",
    "\n",
    "# str2ind_val_dict, ind2str_val_dict = get_str2numb_numb2dict(y_validation)\n",
    "# y_validation = np.array(apply_dict(str2ind_val_dict, y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = model\n",
    "self.num_classes = len(np.unique(y_train))\n",
    "self.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WordsSequence at 0x26e1358e2b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "learning_rate=0.001\n",
    "margin=0.5\n",
    "train_generator = WordsSequence(train_dir, input_shape=self.input_shape, x_set=x_train, y_set=y_train, batch_size=batch_size)\n",
    "# validation_generator = WordsSequence(validation_dir, input_shape=self.input_shape, x_set=validation_pairs, y_set=validation_y, batch_size=batch_size)\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# optimize = RMSprop(lr=learning_rate)\n",
    "optimize = Adam(lr=0.00001)\n",
    "self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.model.compile(loss=triplet_loss(margin=1.0, strategy=\"batch_all\"), optimizer=optimize)\n",
    "\n",
    "self.model.compile(loss=triplet_loss(margin=1.0, strategy=\"batch_semi_hard\"), optimizer=optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "116/116 [==============================] - 923s 8s/step - loss: 0.9483\n",
      "Epoch 2/200\n",
      "116/116 [==============================] - 934s 8s/step - loss: 0.9417\n",
      "Epoch 3/200\n",
      "116/116 [==============================] - 887s 8s/step - loss: 0.9269\n",
      "Epoch 4/200\n",
      "116/116 [==============================] - 904s 8s/step - loss: 0.9363\n",
      "Epoch 5/200\n",
      "116/116 [==============================] - 947s 8s/step - loss: 0.9248\n",
      "Epoch 6/200\n",
      "116/116 [==============================] - 897s 8s/step - loss: 0.9122\n",
      "Epoch 7/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.9112\n",
      "Epoch 8/200\n",
      "116/116 [==============================] - 841s 7s/step - loss: 0.9074\n",
      "Epoch 9/200\n",
      "116/116 [==============================] - 829s 7s/step - loss: 0.9016\n",
      "Epoch 10/200\n",
      "116/116 [==============================] - 932s 8s/step - loss: 0.8888\n",
      "Epoch 11/200\n",
      "116/116 [==============================] - 911s 8s/step - loss: 0.8563\n",
      "Epoch 12/200\n",
      "116/116 [==============================] - 951s 8s/step - loss: 0.8786\n",
      "Epoch 13/200\n",
      "116/116 [==============================] - 929s 8s/step - loss: 0.8561\n",
      "Epoch 14/200\n",
      "116/116 [==============================] - 935s 8s/step - loss: 0.8480\n",
      "Epoch 15/200\n",
      "116/116 [==============================] - 932s 8s/step - loss: 0.8303\n",
      "Epoch 16/200\n",
      "116/116 [==============================] - 867s 7s/step - loss: 0.8451\n",
      "Epoch 17/200\n",
      "116/116 [==============================] - 842s 7s/step - loss: 0.8186\n",
      "Epoch 18/200\n",
      "116/116 [==============================] - 929s 8s/step - loss: 0.8392\n",
      "Epoch 19/200\n",
      "116/116 [==============================] - 934s 8s/step - loss: 0.8299\n",
      "Epoch 20/200\n",
      "116/116 [==============================] - 914s 8s/step - loss: 0.8051\n",
      "Epoch 21/200\n",
      "116/116 [==============================] - 966s 8s/step - loss: 0.7986\n",
      "Epoch 22/200\n",
      "116/116 [==============================] - 927s 8s/step - loss: 0.7947\n",
      "Epoch 23/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.8103\n",
      "Epoch 24/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7959\n",
      "Epoch 25/200\n",
      "116/116 [==============================] - 878s 8s/step - loss: 0.7778\n",
      "Epoch 26/200\n",
      "116/116 [==============================] - 874s 8s/step - loss: 0.7997\n",
      "Epoch 27/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.7788\n",
      "Epoch 28/200\n",
      "116/116 [==============================] - 879s 8s/step - loss: 0.7983\n",
      "Epoch 29/200\n",
      "116/116 [==============================] - 869s 7s/step - loss: 0.7896\n",
      "Epoch 30/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7624\n",
      "Epoch 31/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7647\n",
      "Epoch 32/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.7709\n",
      "Epoch 33/200\n",
      "116/116 [==============================] - 866s 7s/step - loss: 0.7743\n",
      "Epoch 34/200\n",
      "116/116 [==============================] - 859s 7s/step - loss: 0.7592\n",
      "Epoch 35/200\n",
      "116/116 [==============================] - 862s 7s/step - loss: 0.7681\n",
      "Epoch 36/200\n",
      "116/116 [==============================] - 873s 8s/step - loss: 0.7432\n",
      "Epoch 37/200\n",
      "116/116 [==============================] - 880s 8s/step - loss: 0.7502\n",
      "Epoch 38/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.7603\n",
      "Epoch 39/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.7323\n",
      "Epoch 40/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7393\n",
      "Epoch 41/200\n",
      "116/116 [==============================] - 855s 7s/step - loss: 0.7425\n",
      "Epoch 42/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7569\n",
      "Epoch 43/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7371\n",
      "Epoch 44/200\n",
      "116/116 [==============================] - 872s 8s/step - loss: 0.7517\n",
      "Epoch 45/200\n",
      "116/116 [==============================] - 871s 8s/step - loss: 0.7304\n",
      "Epoch 46/200\n",
      "116/116 [==============================] - 854s 7s/step - loss: 0.7268\n",
      "Epoch 47/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.7456\n",
      "Epoch 48/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7312\n",
      "Epoch 49/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.7227\n",
      "Epoch 50/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.7388\n",
      "Epoch 51/200\n",
      "116/116 [==============================] - 866s 7s/step - loss: 0.7044\n",
      "Epoch 52/200\n",
      "116/116 [==============================] - 865s 7s/step - loss: 0.7170\n",
      "Epoch 53/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.7081\n",
      "Epoch 54/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.7088\n",
      "Epoch 55/200\n",
      "116/116 [==============================] - 841s 7s/step - loss: 0.7160\n",
      "Epoch 56/200\n",
      "116/116 [==============================] - 837s 7s/step - loss: 0.7017\n",
      "Epoch 57/200\n",
      "116/116 [==============================] - 845s 7s/step - loss: 0.7023\n",
      "Epoch 58/200\n",
      "116/116 [==============================] - 864s 7s/step - loss: 0.6904\n",
      "Epoch 59/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.6974\n",
      "Epoch 60/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.6905\n",
      "Epoch 61/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.7017\n",
      "Epoch 62/200\n",
      "116/116 [==============================] - 860s 7s/step - loss: 0.6938\n",
      "Epoch 63/200\n",
      "116/116 [==============================] - 859s 7s/step - loss: 0.7007\n",
      "Epoch 64/200\n",
      "116/116 [==============================] - 881s 8s/step - loss: 0.6785\n",
      "Epoch 65/200\n",
      "116/116 [==============================] - 854s 7s/step - loss: 0.6720\n",
      "Epoch 66/200\n",
      "116/116 [==============================] - 857s 7s/step - loss: 0.6797\n",
      "Epoch 67/200\n",
      "116/116 [==============================] - 853s 7s/step - loss: 0.6867\n",
      "Epoch 68/200\n",
      "116/116 [==============================] - 852s 7s/step - loss: 0.6730\n",
      "Epoch 69/200\n",
      "116/116 [==============================] - 850s 7s/step - loss: 0.6888\n",
      "Epoch 70/200\n",
      "116/116 [==============================] - 847s 7s/step - loss: 0.6659\n",
      "Epoch 71/200\n",
      "116/116 [==============================] - 851s 7s/step - loss: 0.6725\n",
      "Epoch 72/200\n",
      "116/116 [==============================] - 855s 7s/step - loss: 0.6765\n",
      "Epoch 73/200\n",
      "116/116 [==============================] - 877s 8s/step - loss: 0.6533\n",
      "Epoch 74/200\n",
      "116/116 [==============================] - 863s 7s/step - loss: 0.6498\n",
      "Epoch 75/200\n",
      "116/116 [==============================] - 840s 7s/step - loss: 0.6466\n",
      "Epoch 76/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.6562\n",
      "Epoch 77/200\n",
      "116/116 [==============================] - 849s 7s/step - loss: 0.6381\n",
      "Epoch 78/200\n",
      "116/116 [==============================] - 856s 7s/step - loss: 0.6404\n",
      "Epoch 79/200\n",
      "116/116 [==============================] - 890s 8s/step - loss: 0.6415\n",
      "Epoch 80/200\n",
      "116/116 [==============================] - 873s 8s/step - loss: 0.6413\n",
      "Epoch 81/200\n",
      "116/116 [==============================] - 865s 7s/step - loss: 0.6686\n",
      "Epoch 82/200\n",
      "116/116 [==============================] - 1083s 9s/step - loss: 0.6602\n",
      "Epoch 83/200\n",
      "116/116 [==============================] - 869s 7s/step - loss: 0.6530\n",
      "Epoch 84/200\n",
      "116/116 [==============================] - 826s 7s/step - loss: 0.6636\n",
      "Epoch 85/200\n",
      "116/116 [==============================] - 837s 7s/step - loss: 0.6244\n",
      "Epoch 86/200\n",
      "116/116 [==============================] - 819s 7s/step - loss: 0.6315\n",
      "Epoch 87/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.6491\n",
      "Epoch 88/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.6409\n",
      "Epoch 89/200\n",
      "116/116 [==============================] - 805s 7s/step - loss: 0.6263\n",
      "Epoch 90/200\n",
      "116/116 [==============================] - 810s 7s/step - loss: 0.6232\n",
      "Epoch 91/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.6082\n",
      "Epoch 92/200\n",
      "116/116 [==============================] - 806s 7s/step - loss: 0.6095\n",
      "Epoch 93/200\n",
      "116/116 [==============================] - 812s 7s/step - loss: 0.6073\n",
      "Epoch 94/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5897\n",
      "Epoch 95/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.5961\n",
      "Epoch 96/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.6157\n",
      "Epoch 97/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.6144\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 802s 7s/step - loss: 0.6098\n",
      "Epoch 99/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.6043\n",
      "Epoch 100/200\n",
      "116/116 [==============================] - 814s 7s/step - loss: 0.5937\n",
      "Epoch 101/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.5902\n",
      "Epoch 102/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.5734\n",
      "Epoch 103/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.5993\n",
      "Epoch 104/200\n",
      "116/116 [==============================] - 803s 7s/step - loss: 0.5787\n",
      "Epoch 105/200\n",
      "116/116 [==============================] - 810s 7s/step - loss: 0.5742\n",
      "Epoch 106/200\n",
      "116/116 [==============================] - 775s 7s/step - loss: 0.5727\n",
      "Epoch 107/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.5981\n",
      "Epoch 108/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5810\n",
      "Epoch 109/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5625\n",
      "Epoch 110/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.5930\n",
      "Epoch 111/200\n",
      "116/116 [==============================] - 803s 7s/step - loss: 0.5601\n",
      "Epoch 112/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.6001\n",
      "Epoch 113/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5627\n",
      "Epoch 114/200\n",
      "116/116 [==============================] - 807s 7s/step - loss: 0.5715\n",
      "Epoch 115/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5710\n",
      "Epoch 116/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.5493\n",
      "Epoch 117/200\n",
      "116/116 [==============================] - 800s 7s/step - loss: 0.5584\n",
      "Epoch 118/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5570\n",
      "Epoch 119/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.5416\n",
      "Epoch 120/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5345\n",
      "Epoch 121/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.5583\n",
      "Epoch 122/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.5584\n",
      "Epoch 123/200\n",
      "116/116 [==============================] - 806s 7s/step - loss: 0.5661\n",
      "Epoch 124/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.5558\n",
      "Epoch 125/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.5304\n",
      "Epoch 126/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5271\n",
      "Epoch 127/200\n",
      "116/116 [==============================] - 796s 7s/step - loss: 0.5199\n",
      "Epoch 128/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.5088\n",
      "Epoch 129/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.5156\n",
      "Epoch 130/200\n",
      "116/116 [==============================] - 804s 7s/step - loss: 0.5240\n",
      "Epoch 131/200\n",
      "116/116 [==============================] - 797s 7s/step - loss: 0.5084\n",
      "Epoch 132/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.5240\n",
      "Epoch 133/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.5145\n",
      "Epoch 134/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.5009\n",
      "Epoch 135/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.5285\n",
      "Epoch 136/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.5256\n",
      "Epoch 137/200\n",
      "116/116 [==============================] - 808s 7s/step - loss: 0.4942\n",
      "Epoch 138/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.5101\n",
      "Epoch 139/200\n",
      "116/116 [==============================] - 780s 7s/step - loss: 0.4926\n",
      "Epoch 140/200\n",
      "116/116 [==============================] - 775s 7s/step - loss: 0.5080\n",
      "Epoch 141/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.5083\n",
      "Epoch 142/200\n",
      "116/116 [==============================] - 780s 7s/step - loss: 0.4982\n",
      "Epoch 143/200\n",
      "116/116 [==============================] - 776s 7s/step - loss: 0.4858\n",
      "Epoch 144/200\n",
      "116/116 [==============================] - 778s 7s/step - loss: 0.4872\n",
      "Epoch 145/200\n",
      "116/116 [==============================] - 781s 7s/step - loss: 0.5040\n",
      "Epoch 146/200\n",
      "116/116 [==============================] - 778s 7s/step - loss: 0.4760\n",
      "Epoch 147/200\n",
      "116/116 [==============================] - 781s 7s/step - loss: 0.4931\n",
      "Epoch 148/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.4568\n",
      "Epoch 149/200\n",
      "116/116 [==============================] - 783s 7s/step - loss: 0.4800\n",
      "Epoch 150/200\n",
      "116/116 [==============================] - 812s 7s/step - loss: 0.4846\n",
      "Epoch 151/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4763\n",
      "Epoch 152/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.4652\n",
      "Epoch 153/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4844\n",
      "Epoch 154/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4705\n",
      "Epoch 155/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4607\n",
      "Epoch 156/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4699\n",
      "Epoch 157/200\n",
      "116/116 [==============================] - 807s 7s/step - loss: 0.4943\n",
      "Epoch 158/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.4797\n",
      "Epoch 159/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4730\n",
      "Epoch 160/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4546\n",
      "Epoch 161/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4608\n",
      "Epoch 162/200\n",
      "116/116 [==============================] - 785s 7s/step - loss: 0.4483\n",
      "Epoch 163/200\n",
      "116/116 [==============================] - 786s 7s/step - loss: 0.4360\n",
      "Epoch 164/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4494\n",
      "Epoch 165/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.4513\n",
      "Epoch 166/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4440\n",
      "Epoch 167/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.4512\n",
      "Epoch 168/200\n",
      "116/116 [==============================] - 811s 7s/step - loss: 0.4222\n",
      "Epoch 169/200\n",
      "116/116 [==============================] - 799s 7s/step - loss: 0.4156\n",
      "Epoch 170/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4328\n",
      "Epoch 171/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4486\n",
      "Epoch 172/200\n",
      "116/116 [==============================] - 793s 7s/step - loss: 0.4599\n",
      "Epoch 173/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.4394\n",
      "Epoch 174/200\n",
      "116/116 [==============================] - 782s 7s/step - loss: 0.4160\n",
      "Epoch 175/200\n",
      "116/116 [==============================] - 818s 7s/step - loss: 0.4389\n",
      "Epoch 176/200\n",
      "116/116 [==============================] - 805s 7s/step - loss: 0.4269\n",
      "Epoch 177/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4187\n",
      "Epoch 178/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.4038\n",
      "Epoch 179/200\n",
      "116/116 [==============================] - 823s 7s/step - loss: 0.4312\n",
      "Epoch 180/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.3996\n",
      "Epoch 181/200\n",
      "116/116 [==============================] - 795s 7s/step - loss: 0.4247\n",
      "Epoch 182/200\n",
      "116/116 [==============================] - 802s 7s/step - loss: 0.4012\n",
      "Epoch 183/200\n",
      "116/116 [==============================] - 790s 7s/step - loss: 0.4203\n",
      "Epoch 184/200\n",
      "116/116 [==============================] - 824s 7s/step - loss: 0.4246\n",
      "Epoch 185/200\n",
      "116/116 [==============================] - 789s 7s/step - loss: 0.3972\n",
      "Epoch 186/200\n",
      "116/116 [==============================] - 798s 7s/step - loss: 0.4068\n",
      "Epoch 187/200\n",
      "116/116 [==============================] - 815s 7s/step - loss: 0.3751\n",
      "Epoch 188/200\n",
      "116/116 [==============================] - 788s 7s/step - loss: 0.3941\n",
      "Epoch 189/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.3677\n",
      "Epoch 190/200\n",
      "116/116 [==============================] - 818s 7s/step - loss: 0.3782\n",
      "Epoch 191/200\n",
      "116/116 [==============================] - 819s 7s/step - loss: 0.3768\n",
      "Epoch 192/200\n",
      "116/116 [==============================] - 794s 7s/step - loss: 0.3765\n",
      "Epoch 193/200\n",
      "116/116 [==============================] - 801s 7s/step - loss: 0.4412\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 807s 7s/step - loss: 0.3910\n",
      "Epoch 195/200\n",
      "116/116 [==============================] - 809s 7s/step - loss: 0.3616\n",
      "Epoch 196/200\n",
      "116/116 [==============================] - 784s 7s/step - loss: 0.3842\n",
      "Epoch 197/200\n",
      "116/116 [==============================] - 791s 7s/step - loss: 0.3763\n",
      "Epoch 198/200\n",
      "116/116 [==============================] - 816s 7s/step - loss: 0.3821\n",
      "Epoch 199/200\n",
      "116/116 [==============================] - 792s 7s/step - loss: 0.3795\n",
      "Epoch 200/200\n",
      "116/116 [==============================] - 813s 7s/step - loss: 0.3943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26efbe7bf98>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_data=validation_generator, \n",
    "self.model.fit_generator(train_generator, shuffle=True, epochs=200, verbose=1, \n",
    "callbacks=[ModelCheckpoint(filepath=os.path.join(self.cache_dir, 'checkpoint-{epoch:02d}.h5'), save_weights_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = 'Literature Data/triplet_cache/my_final_model.h5'\n",
    "self.model.save(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_weights = self.cache_dir + '/' + 'my_final_weights.h5'\n",
    "self.save_weights(path_to_save_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = train_dir\n",
    "test_csv = 'Literature data/test.csv'\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeap\n"
     ]
    }
   ],
   "source": [
    "if self.embeddings is not None:\n",
    "    print(self.embeddings[0][0])\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "else:\n",
    "    print('Yeap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 178s 2s/step\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv)\n",
    "x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "\n",
    "self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "pred = self.model.predict_generator(words, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00762717e-01,  1.38824418e-01, -1.99312828e-02, ...,\n",
       "         3.62386629e-02,  6.29772060e-03,  1.07450590e-01],\n",
       "       [-8.49247426e-02,  1.20439470e-01, -2.18481780e-03, ...,\n",
       "         1.63447205e-02,  9.15719866e-05,  1.10471539e-01],\n",
       "       [-9.62019190e-02,  1.22878075e-01, -9.91626456e-03, ...,\n",
       "         1.99544169e-02,  9.16776992e-03,  1.09453216e-01],\n",
       "       ...,\n",
       "       [-1.09470241e-01,  1.35934681e-01, -1.08547136e-02, ...,\n",
       "         1.89207606e-02,  2.57770298e-03,  1.03421457e-01],\n",
       "       [-9.90877450e-02,  1.07763655e-01, -1.20304301e-02, ...,\n",
       "         9.89381410e-03,  1.19910911e-02,  1.27858549e-01],\n",
       "       [-1.02851026e-01,  1.21404469e-01, -1.88112240e-02, ...,\n",
       "         2.57086288e-02,  1.31449988e-02,  1.19343467e-01]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "self.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.clf.fit(pred, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.10076272, 0.13882442, -0.019931283, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [[-0.10076272, 0.13882442, -0.019931283, 0.003...\n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.save_embeddings('Literature Data/triplet_cache/embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.make_embeddings(train_dir, \"train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"../data/test.csv\", batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'Literature Data/train.csv'\n",
    "img_dir = 'Literature Data/train_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 281s 2s/step\n"
     ]
    }
   ],
   "source": [
    "if self.embeddings is not None:\n",
    "    print(self.embeddings[0][0])\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(self.embeddings[0][0], self.embeddings[0][1])\n",
    "else:\n",
    "    data = pd.read_csv(csv)\n",
    "    x, y = data['file_name'].as_matrix(), data['label'].as_matrix()\n",
    "\n",
    "    self.str2ind_test_dict, self.ind2str_test_dict = get_str2numb_numb2dict(y)\n",
    "    y = np.array(apply_dict(self.str2ind_test_dict, y))\n",
    "\n",
    "    words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x, batch_size=batch_size)\n",
    "    pred = self.model.predict_generator(words, verbose=1)\n",
    "\n",
    "    self.clf = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "    self.clf.fit(pred, y) \n",
    "\n",
    "    self.embeddings =  pd.DataFrame(data=[pred, y])\n",
    "    self.save_embeddings('embeddings.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 3,360,064\n",
      "Trainable params: 3,338,176\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "img_dir = test_dir\n",
    "self.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = 'Literature Data/test.csv'\n",
    "test = pd.read_csv(test_csv)\n",
    "x_test, y_test = test['file_name'].as_matrix(), test['label'].as_matrix()\n",
    "\n",
    "str2ind_test_dict, ind2str_test_dict = get_str2numb_numb2dict(y_test)\n",
    "test_y = np.array(apply_dict(str2ind_test_dict, y_test))\n",
    "\n",
    "words = WordsSequence(img_dir, input_shape=self.input_shape, x_set=x_test, batch_size=batch_size)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 221s 3s/step\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = self.model.predict_generator(words, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word accuracy:  0.19163636363636363\n"
     ]
    }
   ],
   "source": [
    "res = self.clf.predict(test_embeddings) \n",
    "predict = np.array(apply_dict(ind2str_test_dict , res))\n",
    "count = 0\n",
    "for i,j in zip(predict, y_test):\n",
    "    if i == j:\n",
    "        count += 1\n",
    "\n",
    "print('word accuracy: ', count / len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-1 autor accuracy:  0.3611111111111111\n",
      "top-5 autor accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "autors = np.unique(y_test)\n",
    "autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "    if p == autors[i]:\n",
    "        count += 1\n",
    "\n",
    "print('top-1 autor accuracy: ', count / len(autors))\n",
    "\n",
    "count = 0\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = [pair[0] for pair in Counter(np.ravel(predict[inds])).most_common(5)]\n",
    "    if autors[i] in p:\n",
    "        count += 1\n",
    "\n",
    "print('top-5 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1_0    1_20\n",
      "    1_1\n",
      "      1_10    4_22\n",
      "      1_12    3_13\n",
      "    1_13\n",
      "    1_14\n",
      "    1_15\n",
      "      1_16    4_17\n",
      "    1_17\n",
      "    1_18\n",
      "    1_19\n",
      "      1_2    1_22\n",
      "    1_20\n",
      "    1_21\n",
      "    1_22\n",
      "      1_23    5_9\n",
      "    1_24\n",
      "      1_3    4_20\n",
      "    1_4\n",
      "      1_5    1_24\n",
      "    1_6\n",
      "    1_8\n",
      "    1_9\n",
      "    2_0\n",
      "    2_1\n",
      "      2_10    5_16\n",
      "      2_11    3_12\n",
      "      2_12    6_12\n",
      "    2_13\n",
      "    2_14\n",
      "    2_15\n",
      "    2_2\n",
      "      2_3    3_9\n",
      "    2_4\n",
      "    2_5\n",
      "    2_6\n",
      "      2_8    1_10\n",
      "      2_9    5_17\n",
      "    3_0\n",
      "    3_1\n",
      "    3_11\n",
      "    3_12\n",
      "    3_13\n",
      "      3_14    5_0\n",
      "    3_15\n",
      "      3_16    6_9\n",
      "    3_17\n",
      "    3_18\n",
      "    3_19\n",
      "      3_2    4_3\n",
      "    3_3\n",
      "    3_4\n",
      "    3_5\n",
      "      3_6    6_12\n",
      "    3_8\n",
      "    3_9\n",
      "      4_1    4_20\n",
      "      4_11    3_15\n",
      "      4_15    4_17\n",
      "      4_17    4_18\n",
      "      4_18    4_19\n",
      "      4_19    4_1\n",
      "      4_2    4_3\n",
      "      4_20    4_21\n",
      "      4_21    3_12\n",
      "      4_22    4_2\n",
      "      4_3    4_4\n",
      "      4_4    4_5\n",
      "      4_5    4_6\n",
      "      4_6    4_7\n",
      "      4_7    3_15\n",
      "      4_8    4_9\n",
      "      4_9    5_0\n",
      "      5_0    5_10\n",
      "      5_1    5_2\n",
      "      5_10    4_19\n",
      "      5_11    5_12\n",
      "      5_12    5_13\n",
      "      5_13    5_14\n",
      "      5_14    5_15\n",
      "      5_15    5_16\n",
      "      5_16    3_13\n",
      "      5_17    4_9\n",
      "      5_18    5_1\n",
      "      5_2    5_9\n",
      "      5_9    6_10\n",
      "      6_0    6_11\n",
      "      6_10    6_12\n",
      "      6_11    6_6\n",
      "      6_12    6_7\n",
      "      6_6    6_8\n",
      "      6_7    6_9\n",
      "      6_8    7_0\n",
      "      6_9    7_10\n",
      "      7_0    7_11\n",
      "      7_1    7_3\n",
      "      7_10    7_12\n",
      "      7_11    7_13\n",
      "      7_12    7_1\n",
      "      7_13    7_9\n",
      "      7_2    7_4\n",
      "      7_3    7_5\n",
      "      7_4    7_6\n",
      "      7_5    7_7\n",
      "      7_6    7_3\n",
      "      7_7    7_8\n",
      "      7_8    7_3\n",
      "    7_9\n",
      "top-1 autor accuracy:  0.3611111111111111\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "autors = np.unique(y_test)\n",
    "autor_ind = [np.argwhere(y_test == a) for a in autors]\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = Counter(np.ravel(predict[inds])).most_common(1)[0][0]\n",
    "    if p == autors[i]:\n",
    "        print('    ' + str(autors[i]))\n",
    "        count += 1\n",
    "    else:\n",
    "         print('      ' + str(autors[i]) + '    ' + str(p))\n",
    "\n",
    "print('top-1 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1_0    [11, 0, 43, 89, 9]\n",
      "      1_1    [10, 36, 29, 55, 80]\n",
      "      1_10    [64, 1, 105, 38, 107]\n",
      "      1_12    [41, 60, 2, 93, 8]\n",
      "      1_13    [3, 89, 14, 20, 52]\n",
      "      1_14    [4, 33, 5, 44, 52]\n",
      "      1_15    [5, 34, 4, 97, 6]\n",
      "      1_16    [58, 34, 47, 91, 73]\n",
      "      1_17    [7, 54, 45, 26, 59]\n",
      "      1_18    [8, 38, 73, 88, 71]\n",
      "      1_19    [9, 79, 11, 57, 0]\n",
      "      1_2    [13, 12, 92, 80, 71]\n",
      "      1_20    [11, 3, 19, 24, 89]\n",
      "      1_21    [12, 39, 1, 22, 33]\n",
      "      1_22    [13, 79, 43, 3]\n",
      "      1_23    [85, 14, 45, 20, 13]\n",
      "      1_24    [15, 46, 31, 11, 61]\n",
      "      1_3    [62, 76, 51, 17, 32]\n",
      "      1_4    [18, 54]\n",
      "      1_5    [15, 46, 20, 104, 92]\n",
      "      1_6    [20, 85, 72, 46, 8]\n",
      "      1_8    [21, 38, 77, 39, 40]\n",
      "      1_9    [22, 42, 41, 58, 75]\n",
      "      2_0    [23, 93, 94, 43, 24]\n",
      "      2_1    [30, 58, 48, 88, 50]\n",
      "      2_10    [80, 69, 24, 58, 16]\n",
      "      2_11    [40, 106, 88, 96, 25]\n",
      "      2_12    [89, 26, 3, 79, 43]\n",
      "      2_13    [27, 40, 74, 63, 19]\n",
      "      2_14    [28, 69, 84, 15, 47]\n",
      "      2_15    [29, 68, 47, 46, 102]\n",
      "      2_2    [31, 62, 53, 32, 40]\n",
      "      2_3    [55, 32, 53, 58, 17]\n",
      "      2_4    [33, 87, 34, 41, 58]\n",
      "      2_5    [34, 5, 6, 31, 52]\n",
      "      2_6    [35, 6, 53, 48, 4]\n",
      "      2_8    [1, 36, 73, 76, 40]\n",
      "      2_9    [81, 37, 40, 72, 65]\n",
      "      3_0    [38, 8, 60, 40, 3]\n",
      "      3_1    [48, 62, 40, 37, 88]\n",
      "      3_11    [39, 38, 60, 77, 93]\n",
      "      3_12    [40, 88, 38, 60, 43]\n",
      "      3_13    [41, 39, 97, 8, 12]\n",
      "      3_14    [73, 42, 22, 8, 64]\n",
      "      3_15    [43, 11, 13, 79, 41]\n",
      "      3_16    [93, 44, 11, 23, 80]\n",
      "      3_17    [45, 83, 0, 38, 3]\n",
      "      3_18    [46, 84, 91, 28, 90]\n",
      "      3_19    [47, 90, 91, 66, 46]\n",
      "      3_2    [66, 49, 83, 90, 47]\n",
      "      3_3    [50, 29, 88, 30]\n",
      "      3_4    [51, 17, 84, 42, 69]\n",
      "      3_5    [52, 4, 33, 36, 31]\n",
      "      3_6    [89, 3, 9, 7, 93]\n",
      "      3_8    [54, 7, 87, 32, 33]\n",
      "      3_9    [55, 4, 53, 52, 15]\n",
      "      4_1    [62, 55, 40, 88, 81]\n",
      "      4_11    [43, 92, 13, 78, 57]\n",
      "      4_15    [58, 34, 33, 6, 21]\n",
      "      4_17    [59, 95, 93, 27, 56]\n",
      "      4_18    [60, 39, 38, 41, 40]\n",
      "      4_19    [61, 20, 92, 37, 42]\n",
      "      4_2    [66, 49, 83, 90, 47]\n",
      "      4_20    [63, 51, 30, 15, 19]\n",
      "      4_21    [40, 64, 101, 33, 88]\n",
      "      4_22    [65, 17, 95, 37, 28]\n",
      "      4_3    [67, 48, 35, 30, 84]\n",
      "      4_4    [68, 90, 88, 29, 91]\n",
      "      4_5    [69, 24, 10, 82, 29]\n",
      "      4_6    [70, 77, 17, 21, 73]\n",
      "      4_7    [43, 3, 80, 42, 65]\n",
      "      4_8    [72, 82, 40, 69, 92]\n",
      "      4_9    [73, 34, 102, 64, 17]\n",
      "      5_0    [74, 94, 13, 65, 98]\n",
      "      5_1    [84, 69, 85, 28, 46]\n",
      "      5_10    [60, 77, 75, 98, 22]\n",
      "      5_11    [76, 74, 65, 17, 47]\n",
      "      5_12    [77, 70, 12, 36, 48]\n",
      "      5_13    [78, 79, 43, 57, 18]\n",
      "      5_14    [79, 3, 13, 10, 11]\n",
      "      5_15    [80, 38, 21, 77, 60]\n",
      "      5_16    [41, 82, 40, 81, 72]\n",
      "      5_17    [72, 82, 43, 98, 69]\n",
      "      5_18    [83, 0, 11, 90, 50]\n",
      "      5_2    [85, 29, 11, 15, 102]\n",
      "      5_9    [87, 39, 60, 41, 3]\n",
      "      6_0    [88, 40, 30, 50, 33]\n",
      "      6_10    [89, 11, 93, 9, 77]\n",
      "      6_11    [90, 66, 81, 29, 49]\n",
      "      6_12    [91, 47, 15, 90, 84]\n",
      "      6_6    [92, 61, 74, 82, 3]\n",
      "      6_7    [93, 8, 43, 25, 92]\n",
      "      6_8    [94, 79, 14, 82, 80]\n",
      "      6_9    [95, 59, 51, 78, 56]\n",
      "      7_0    [96, 103, 98, 101, 97]\n",
      "      7_1    [101, 68, 104, 107, 30]\n",
      "      7_10    [97, 98, 106, 93, 73]\n",
      "      7_11    [98, 103, 106, 90, 5]\n",
      "      7_12    [99, 100, 104]\n",
      "      7_13    [107, 105, 101, 100, 88]\n",
      "      7_2    [102, 20, 50, 37, 106]\n",
      "      7_3    [103, 100, 104]\n",
      "      7_4    [104, 68, 0]\n",
      "      7_5    [105, 107, 83]\n",
      "      7_6    [101, 105, 96, 1, 107]\n",
      "      7_7    [106, 101, 107, 91]\n",
      "      7_8    [101, 98, 105, 106, 36]\n",
      "      7_9    [107, 105, 34, 101]\n",
      "top-5 autor accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i,inds in enumerate(autor_ind):\n",
    "    p = [pair[0] for pair in Counter(np.ravel(res[inds])).most_common(5)]\n",
    "    if autors[i] in p:\n",
    "        print('    ' + str(autors[i]))\n",
    "        count += 1\n",
    "    else:\n",
    "        print('      ' + str(autors[i]) + '    ' + str(p))\n",
    "\n",
    "print('top-5 autor accuracy: ', count / len(autors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triplet_model import TripletModel\n",
    "\n",
    "train_dir = 'C:/Users/Anastasia/Pictures/words_train'\n",
    "validation_dir = 'C:/Users/Anastasia/Pictures/words_validation'\n",
    "test_dir = '../data/words_test'\n",
    "\n",
    "# Train\n",
    "# model = TripletModel(input_shape=(160, 160, 3), cache_dir=\"triplet_cache_new\")\n",
    "# model.load_weights(\"triplet_cache_new/train_all/checkpoint-06.h5\")\n",
    "# model.train(train_dir, \"train.csv\", validation_dir, \"validation.csv\", epochs=200)\n",
    " \n",
    "# Predict\n",
    "model = TripletModel(alpha=0.75, input_shape=(160, 160, 3), cache_dir=\"triplet_cache\")\n",
    "model.load_weights(\"final_weigths_alpha_0.75/final.h5\")\n",
    "model.load_embeddings('../data/triplet_embeddings_75.pkl')\n",
    "model.make_embeddings(train_dir, \"train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"../data/test.csv\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 24)        216       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 48)        1152      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 96)        4608      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 192)       18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 384)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 384)         3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 768)         294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 1,931,408\n",
      "Trainable params: 1,914,992\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08118027 -0.00346136  0.07533943 ...  0.01582423  0.14860386\n",
      "  -0.05757323]\n",
      " [ 0.06136449  0.04853927  0.046064   ...  0.00425277  0.06847595\n",
      "   0.04984679]\n",
      " [ 0.09801209  0.02759981  0.05831361 ...  0.07109818  0.11770546\n",
      "  -0.01831287]\n",
      " ...\n",
      " [ 0.04524183 -0.02268627  0.08047596 ...  0.1392696   0.14579052\n",
      "   0.0180465 ]\n",
      " [ 0.02804308  0.01359361  0.11527269 ...  0.10519566  0.11367899\n",
      "   0.03562412]\n",
      " [ 0.09554158  0.01088285  0.0726615  ...  0.1196068   0.14568764\n",
      "   0.03558488]]\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 161, 161, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 24)        648       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 24)        216       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 24)        96        \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 80, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 48)        1152      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 80, 80, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 81, 81, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 48)        432       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 40, 40, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 96)        4608      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 96)        9216      \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 40, 40, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 41, 41, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 96)        864       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 192)       18432     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 192)       36864     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 21, 21, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 192)       1728      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 10, 10, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 384)       73728     \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 384)       3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 384)       147456    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 10, 10, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 11, 11, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 384)         3456      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 768)         294912    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 768)         6912      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 768)         589824    \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 768)         3072      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 5, 5, 768)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 1,931,408\n",
      "Trainable params: 1,914,992\n",
      "Non-trainable params: 16,416\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:160: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607/1607 [==============================] - 61s 38ms/step\n",
      "word accuracy:  0.0043559427504667085\n",
      "top-5 autor accuracy:  0.0\n",
      "top-5 autor accuracy:  0.042105263157894736\n"
     ]
    }
   ],
   "source": [
    "test_dir = 'writer_identification-master/data/words_test'\n",
    "model = TripletModel(alpha=0.75, input_shape=(160, 160, 3), cache_dir=\"triplet_cache\")\n",
    "model.load_weights('final.h5')\n",
    "model.load_embeddings('writer_identification-master/data/triplet_embeddings_75.pkl')\n",
    "model.make_embeddings(train_dir, \"writer_identification-master/data/train.csv\", batch_size=1)\n",
    "model.predict(test_dir, \"writer_identification-master/data/test.csv\", batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
